@article{PANT2021103275,
title = {FADS: A framework for autonomous drone safety using temporal logic-based trajectory planning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {130},
pages = {103275},
year = {2021},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2021.103275},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X21002874},
author = {Yash Vardhan Pant and Max Z. Li and Alena Rodionova and Rhudii A. Quaye and Houssam Abbas and Megan S. Ryerson and Rahul Mangharam},
keywords = {Unmanned Aerial Systems, Urban Air Mobility, Signal Temporal Logic, Robust Trajectory Lanning},
abstract = {In this work, we present an integrated Framework for Autonomous Drone Safety (FADS). The demand for safe and efficient mobility of people and goods is growing rapidly, in line with the growth in population in US urban centers. In response, new technologies to meet these urban mobility demands are also rapidly maturing in preparation for future full-scale deployment. As surface congestion increases and the technology surrounding unmanned aerial systems (UAS) matures, more people are looking to the urban airspace and Urban Air Mobility (UAM) as a piece of the puzzle to promote mobility in cities. However, the lack of coordination between UAS stakeholders, federal UAS safety regulations, and researchers developing UAS algorithms continues to be a critical barrier to widespread UAS adoption. FADS takes into account federal UAS safety requirements, UAM challenge scenarios, contingency events, as well as stakeholder-specific operational requirements. FADS formalizes these requirements, through Signal Temporal Logic (STL) representations, and a trajectory planning optimization for multi-rotor UAS fleets guarantees robust and continuous-time satisfaction of the requirements and mission objectives. The intuitive FADS user interface makes it easy to plan missions in a variety of environments; we demonstrate this through several rural and urban environment-based case studies. FADS holistically integrates high-level stakeholder objectives with low-level trajectory planning; combined with a user-friendly interface, FADS reduces the complexity of stakeholder coordination within the UAM context.}
}
@article{QIAO2024105915,
title = {A transformer-based neural network for ignition location prediction from the final wildfire perimeter},
journal = {Environmental Modelling & Software},
volume = {172},
pages = {105915},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105915},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223003018},
author = {Yuming Qiao and Wenyu Jiang and Guofeng Su and Juncai Jiang and Xin Li and Fei Wang},
keywords = {Wildfire incident, Ignition location prediction, Deep learning, Transformer model},
abstract = {Ignition location prediction is crucial for wildfire incident investigation and events reconstruction. However, existing models mainly focus on simulating the wildfire forward and rarely trace the ignition backward. In this paper, a novel transformer-based neural network named ILNet was proposed to predict the ignition location backward from the final wildfire perimeter. The ILNet first concatenated all wildfire-driven data as a composite image and divided it into several regular patches. Then, the self-attention mechanism was adopted to extract global spatial features with a variable scale among these patches. These features were further decoded to output semantic masks of growth phase and ignition phase. The geometric center of ignition phase was defined as the ignition location. Finally, a real wildfire was chosen as the study case. The results show the competitive performance of ILNet model (MIoU: 88.45%, IDE_N: 1.99%, computation time: 0.57s), enabling to improve the traditional field work for government agencies.}
}
@incollection{ELSHIRBENY20241,
title = {Chapter 1 - Advances in earth observation and artificial intelligence in monitoring vegetation dynamics of dryland agroecosystems},
editor = {Dipanwita Dutta and Arnab Kundu and N.R. Patel},
booktitle = {Vegetation Dynamics and Crop Stress},
publisher = {Academic Press},
pages = {1-19},
year = {2024},
isbn = {978-0-323-95616-1},
doi = {https://doi.org/10.1016/B978-0-323-95616-1.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956161000018},
author = {Mohammed A. El-Shirbeny and Chandrashekhar Biradar},
keywords = {Remote sensing, drylands, agroecosystems, drones, robots, smart farming, machine learning},
abstract = {Earth observation (EO) and artificial intelligence (AI) have infiltrated the world’s most important sectors, including agriculture, the backbone of the global economy. The vegetation dynamics affecting the agroecosystems in drylands. By 2050 population growth will need a 50% increase in food consumption. EO and AI can assist farmers in improving crop quality and resource usage. Significant investment has been made in developing self-driving vehicles for various applications, including self-driving farm tractors. This gadget integrates GPS and automation to assist tractors in avoiding excessive soil pressure. It facilitates planting and harvesting while also saving time by designing harvest paths. These tractors are built to identify areas in fields to determine the frequency of plowing and to avoid obstacles, such as irrigation equipment, persons, and animals. Self-driving tractors would change agriculture if enough agricultural training data were accessible. According to the FAO, pests, and insects destroy 20%–40% of the world’s grain supply. Before crops are harvested and stored, pests and insects pose the greatest threat. When locusts are expected in a field, AI informs farmers through mobile phones. Satellite data are analyzed and compared using algorithms. This helps farmers prevent more damage and remove costly bugs. AI firms are developing agricultural robots. These robots can be weeding, harvest crops quicker than people, assess crop quality, and identify plants and weeds. It can collect approximately 30,000 m2 every day. This saves time, effort, and human resources. The land and crops of food-producing countries are likewise a concern. Deep learning is used in several applications to detect soil flaws, nutritional deficiencies, and infections. Smartphone photographs might be used to identify plant concerns. In the form of in-depth films, the show provides guidance and other feasible solutions. Farmers may be able to do soil assessments thanks to machine learning (ML). These systems keep track of soil conditions and healthy crops to ensure a healthy and bountiful harvest. Aerial photographs were obtained by drone for crop health monitoring. Drones are used to collect data from agricultural regions, which are then downloaded by USB and examined by experts. Because plants are susceptible to mold and bacteria, the company uses algorithms to analyze gathered images and provides a comprehensive report on the plants’ health and leaf condition. This enables growers to manage plant health and protect them immediately. In agriculture, AI has grown more accurate and controlled by advising farmers on optimal planting, water management, and timely harvesting. Using satellite and drone imagery, AI tools make predictions. It uses temperature, precipitation, wind speed, and sunlight to evaluate agricultural sustainability, plant nutrition, and disease or pest presence. Weather conditions influence these assessments. AI enables farmers to automate their work and conduct precision farming, resulting in higher agricultural yields with less labor and resources. Agriculture enterprises based on AI and ML will attain leadership and technological advancement in the future. These companies will provide the most effective applications to assist the world with food production due to the yearly population increase.}
}
@article{SATHIANARAYANAN2024104352,
title = {Extracting disaster location identification from social media images using deep learning},
journal = {International Journal of Disaster Risk Reduction},
volume = {104},
pages = {104352},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104352},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924001146},
author = {Manikandan Sathianarayanan and Pai-Hui Hsu and Chy-Chang Chang},
keywords = {Disaster management, Social media, Convolutional neural network (CNN), Object detection, Phone numbers},
abstract = {Social media platforms have recently played a predominant role in collecting and sharing reliable, timely information for disaster assessment and management. Social media information is available in either image or text format associated with their geographic location (geotagged), and meaningful information can be mined from these multimodal data to allow situational awareness and enhance decision-making during disasters. In order to extract geographic location from an image in which a disaster happens while geotagging information in conjunction with images on social media is not obtainable generally, our proposed framework is designed to extract geographic location from an image by using phone numbers and includes several key modules: image collection, phone number detection, and Google Maps API for location extraction. In this study, manually annotated multi-digit phone numbers dataset along with the Street View House Numbers (SVHN) dataset were used to train a convolutional neural network (CNN) based detection model (i.e., the RetinaNet object detection algorithm) to locate and detect the multi-digit phone numbers from an image. Experimental results indicated that the detection model for phone number detection achieved more than 79% (0.79) Average Precision (AP) of all digits and a reasonable mean Average Precision (mAP) of 82% (0.8248) with an IoU (Intersection over Union) threshold of 0.5. Google Maps API can provide location information based on the phone numbers extracted from object detectors with less distortion in the distance. These results demonstrated the effectiveness of our proposed approach, and it can be utilized in any type of disaster event, such as earthquakes, flooding, wildfires, etc., for improving situation awareness, disaster assessment, and management.}
}
@article{ZHAO2024,
title = {Rapid assessment of large-scale urban destruction in conflict zones using hypergraph-based visual-structural machine learning},
journal = {Journal of Engineering Research},
year = {2024},
issn = {2307-1877},
doi = {https://doi.org/10.1016/j.jer.2024.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S2307187724002189},
author = {Xinjie Zhao and So Morikawa},
keywords = {Graph Neural Networks, Machine Learning, Spatio-Temporal Computation, Damage Assessment, Urban Infrastructure},
abstract = {In an era marked by increasing geopolitical conflicts, the accurate and swift assessment of urban damage in war-affected areas is crucial for implementing effective emergency responses and sustainable recovery planning. Traditional methodologies often fall short in capturing the unpredictable dynamics of warfare and the complex structures of urban systems. This paper introduces a novel visual-graph machine learning framework, ViT2G, that synergizes the feature representation capabilities of Vision Transformers (ViT) with the structural processing strengths of Graph Neural Networks (GNNs). By integrating high-resolution satellite imagery with urban topology through a hypergraph-based data structure and designing three hypergraph-based machine learning models: Hypergraph Convolutional Network (HGCN), Hypergraph Attention Network (HGAT), and Hypergraph Transformer (HGT), we redefine the challenge of assessing war-induced urban destruction as a graph machine learning task. The experiments demonstrate that for the task of patch-wise damage status classification, this framework requires only a pair of pre- and post-war images to perform fine-grained categorization of the damaged areas. In the binary classification task, the Hypergraph Attention Network (HGAT) model achieved a best accuracy of 94.4 %, while in the multi-class classification task, it achieved a best accuracy of 73.4 %, which significantly outperform traditional computer vision models.}
}
@article{CHENG2024127975,
title = {Visual fire detection using deep learning: A survey},
journal = {Neurocomputing},
volume = {596},
pages = {127975},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127975},
url = {https://www.sciencedirect.com/science/article/pii/S092523122400746X},
author = {Guangtao Cheng and Xue Chen and Chenyi Wang and Xiaobo Li and Baoyi Xian and Hao Yu},
keywords = {Fire detection, Deep learning, Fire classification, Fire location, Fire segmentation},
abstract = {Visual Fire Detection (VFD), through the rapid and accurate identification of smoke and flame in images and videos, is crucial for early fire warning and reducing fire hazards. In recent years, the introduction of deep learning has significantly advanced this field, especially in the automatic extraction of discriminative features necessary for VFD. This paper provides a comprehensive review of the latest technological advancements in fire detection using deep learning, offering a broad perspective. Initially, it details the publicly available benchmark datasets widely used in VFD research and the corresponding evaluation metrics, providing a basis for researchers to assess the performance of various algorithms. Subsequently, we propose a systematic categorization framework, dividing VFD tasks into three key directions: fire classification, fire localization, and fire segmentation. For these directions, we thoroughly review the innovative improvements in deep learning models tailored for image and video inputs and discusses how these advancements enhance the accuracy and efficiency of fire detection. Finally, we highlight the challenges in the field and explore future research directions, intending to inspire and guide both newcomers and seasoned researchers in this area.}
}
@article{HARKAT2025115881,
title = {Early fire detection using wavelet based features},
journal = {Measurement},
volume = {242},
pages = {115881},
year = {2025},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2024.115881},
url = {https://www.sciencedirect.com/science/article/pii/S0263224124017664},
author = {Houda Harkat and Hasmath Farhana Thariq Ahmed and José M.P. Nascimento and Alexandre Bernardino},
keywords = {Wildfire, Wavelet-based features, Feature selection, Support Vector Machine (SVM), Radial Basis Function (RBF)},
abstract = {In recent years, millions of hectares of vegetation worldwide have been destroyed by wildfires and forest fires. Computer vision-based fire classification, which separates pixels from image or video datasets into fire and non-fire categories, has gained more attention recently due to technological innovations. Fire pixels in an image or video can be classified using either a deep learning strategy or a traditional machine learning approach. Deep learning algorithms could process enormous volumes of data, but their training model performance is constrained because they fail to consider the differences in complexity between training samples. Moreover, it is not obvious to train a deep learning model without a dedicated GPU unit. Similarly, deep learning techniques that have a scarcity of training data and insufficient features exhibit poor performance in intricate real-world fire situations. Consequently, to categorize fire and non-fire pixels from the processed photos from the publicly accessible datasets, Corsican and FLAME, as well as the aerial private dataset Firefront_Gestosa, the current study uses a lightweight technique based on SVM and a refined set of features. The present research implemented a novel framework for fire detection and classification from a variety of RGB and Infra-red images acquired during real missions, addressing the significant requirement for swift and accurate recognition of diverse types of flames, ranging from wildfires to industrial and domestic fires. The framework employs wavelet decomposition-based features, including wavelet length, standard deviation, variance, energy, and Shannon’s entropy, extracted through a sliding window sampling method within a machine learning approach. It should be noted that managing multidimensional data to train a model is difficult in machine learning applications. This issue is solved adopting a feature selection approach, which eliminates redundant or unnecessary data that affects the functionality of the model. Thus, to enhance model performance, feature selection using ranking algorithms based on theoretical mutual information is applied in combination to the Support Vector Machine (SVM) with Radial Basis Function (RBF) kernel. Extensive experiments demonstrate exceptional results, notably with Haar wavelets, achieving an impressive overall accuracy of 99.43% and remarkable performance across specificity, precision, recall, F-measure, and G-mean metrics. Thus, the present study showcases the potential of advanced image processing techniques to significantly advance fire detection and classification, thereby contributing to fire prevention, management, and research in various contexts.}
}
@article{REIS2023110362,
title = {Detection of forest fire using deep convolutional neural networks with transfer learning approach},
journal = {Applied Soft Computing},
volume = {143},
pages = {110362},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110362},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623003800},
author = {Hatice Catal Reis and Veysel Turk},
keywords = {Artificial intelligence, Computer vision, Deep learning, Hyper-parameter optimization, Transfer learning, Wildfires detection},
abstract = {Forest fires caused by natural causes such as climate change, temperature increase, lightning strikes, volcanic activity or human effects are among the world’s most dangerous, deadly, and destructive disasters. Detection, prevention, and extinguishing forest fires is challenging. In addition, forest fires can cause habitat destruction that cannot be controlled in time and cause great material and moral losses. Therefore, fast and accurate Detection of forest fires is vital in emergency response. Here, in solving the problem, the transfer learning method from deep learning sub-topics can be used, which allows the application of pre-trained networks to a new problem. The Fire Luminosity Airborne-based Machine learning Evaluation dataset (consisting of forest fire images) obtained by Unmanned Aerial Vehicle was used in this study. In the Detection of forest fire images in the dataset, InceptionV3, DenseNet121, ResNet50V2, NASNetMobile, VGG-19 deep learning algorithms, transfer learning techniques that can produce more successful results than networks trained from scratch, and hybrid proposed with Support Vector Machine, Random Forest, Bidirectional Long Short-Term Memory, Gated Recurrent Unit algorithms methods have been applied. In the classification study with the Fire Luminosity Airborne-based Machine learning Evaluation dataset in performance measurement, 97.95% accuracy was obtained from the DenseNet121 model, which was started with random weights. In the transfer learning study using ImageNet weights, satisfactory results were obtained with 99.32% accuracy in the DenseNet121 model. We anticipate that working in forest fire detection and response can be entirely satisfactory.}
}
@article{BURGUES2020141172,
title = {Environmental chemical sensing using small drones: A review},
journal = {Science of The Total Environment},
volume = {748},
pages = {141172},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.141172},
url = {https://www.sciencedirect.com/science/article/pii/S004896972034701X},
author = {Javier Burgués and Santiago Marco},
keywords = {Unmanned aircraft systems, Remotely piloted aircraft systems, Chemical sensors, Gas sensors, Environmental monitoring, Industrial emission monitoring},
abstract = {Recent advances in miniaturization of chemical instrumentation and in low-cost small drones are catalyzing exponential growth in the use of such platforms for environmental chemical sensing applications. The versatility of chemically sensitive drones is reflected by their rapid adoption in scientific, industrial, and regulatory domains, such as in atmospheric research studies, industrial emission monitoring, and in enforcement of environmental regulations. As a result of this interdisciplinarity, progress to date has been reported across a broad spread of scientific and non-scientific databases, including scientific journals, press releases, company websites, and field reports. The aim of this paper is to assemble all of these pieces of information into a comprehensive, structured and updated review of the field of chemical sensing using small drones. We exhaustively review current and emerging applications of this technology, as well as sensing platforms and algorithms developed by research groups and companies for tasks such as gas concentration mapping, source localization, and flux estimation. We conclude with a discussion of the most pressing technological and regulatory limitations in current practice, and how these could be addressed by future research.}
}
@article{SAKTI2024e03101,
title = {Modeling Proboscis monkey conservation sites on Borneo using ensemble machine learning},
journal = {Global Ecology and Conservation},
volume = {54},
pages = {e03101},
year = {2024},
issn = {2351-9894},
doi = {https://doi.org/10.1016/j.gecco.2024.e03101},
url = {https://www.sciencedirect.com/science/article/pii/S2351989424003056},
author = {Anjar Dimara Sakti and Kurnia Putri Adillah and Cokro Santoso and Ismail Al Faruqi and Vempi Satriya Adi Hendrawan and Parwati Sofan and  Rustam and Adam Irwansyah Fauzi and Yudi Setiawan and Inggita Utami and Alinda F.M. Zain and Muhammad Kamal},
keywords = {Habitat suitability, Proboscis monkeys, Machine learning, Remote sensing, Borneo},
abstract = {This study aimed to analyze the habitat suitability of the endangered Proboscis monkey (Nasalis larvatus) on Borneo using a multi-machine-learning approach. This study integrated physical, vegetational, meteorological, and human activity data to develop a comprehensive habitat suitability model. Four machine-learning algorithms, namely, maximum entropy (MaxEnt), random forest (RF), support vector machine (SVM), gradient tree boosting (GTB), and classification and regression trees (CART), were employed to model the habitat suitability index. A total of 1943 sample points were divided into training (70 %) and validation (30 %) sets for the analysis. This study included three main stages: geospatial database creation, spatial habitat modeling using multi-machine-learning algorithms, and habitat suitability evaluation. In addition, the pressure from human development on the habitat suitability index model was analyzed. This study identified a high level of suitability for Proboscis monkey habitats in nearshore areas. The maximum habitat suitability for Proboscis monkeys was observed to be 11.54 %, as evidenced by the consensus of the MaxEnt value and four machine-learning algorithms. Conversely, the minimum habitat suitability was recorded at 13.27 %, as indicated by disagreement among all algorithms. The AUC values for the machine-learning models ranged from 74 % to 90 %, indicating moderate to high predictive performance. This study provides valuable insights for the formulation of well-planned development programs for Proboscis monkeys. The results of this study will contribute to the accurate identification of potential Proboscis monkey habitats, thereby providing support for conservation efforts aimed at safeguarding this endangered species.}
}
@article{PHAM2024102392,
title = {Classifying forest cover and mapping forest fire susceptibility in Dak Nong province, Vietnam utilizing remote sensing and machine learning},
journal = {Ecological Informatics},
volume = {79},
pages = {102392},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102392},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123004211},
author = {Van The Pham and Tuyet Anh Thi Do and Hau Duc Tran and Anh Ngoc Thi Do},
keywords = {XGBoost, Forest fire, ABC-ANFIS, Unmanned aerial vehicle, Bamboo and dipterocarp forests},
abstract = {Forest fires can cause significant harm to the biodiversity, air quality, economy, and industries that depend on forests, which is particularly critical given the current climate change scenario. Therefore, it is crucial to predict potential fire risks, especially in Dak Nong, a province located in the highland region of Vietnam, where forest farming is prevalent. In this study, the utilization of remote sensing data from unmanned aerial vehicles (UAV) revealed a decline in forest areas while agricultural rubber and industrial crops witnessed an increase. Furthermore, the integration of SPOT satellite images with UAV and the eXtreme gradient boosting (XGBoost) classification algorithm proved highly efficient in mapping forest types in this province (OA = 81.446%, and Kappa = 0.803). To assess forest fire susceptibility in this study, a hybrid model known as Artificial bee colony-Adaptive neuro fuzzy inference system (ABC-ANFIS) was employed, which indicated that the majority of forests in the area face high to very high of fires, particularly in bamboo and dipterocarp forest areas. The present model is proposed to suggest better prevention and suppression strategies. These facts can help managers stop fires or deal with this disturbance more effectively that may occur in the future. Policymakers and researchers in different areas, such as the limestone forest in the north or the Dipterocarp forests in Vietnam, may consider replicating this study to evaluate specific wildfire risks and develop appropriate fire prevention strategies tailored to their local circumstances.}
}
@article{CHATURVEDI2022158,
title = {A survey on vision-based outdoor smoke detection techniques for environmental safety},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {185},
pages = {158-187},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2022.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S092427162200017X},
author = {Shubhangi Chaturvedi and Pritee Khanna and Aparajita Ojha},
keywords = {Outdoor smoke, Smoke detection, Smoke classification, Smoke segmentation, Smoke bounding box detection},
abstract = {Early stage smoke detection using image and video analysis is an important area of research due to its enormous applications in mitigating fire hazards and ensuring environmental safety. Numerous solutions have been proposed for real-time smoke detection using conventional image processing, machine learning, and deep learning techniques. Smoke pattern, motion analysis, color and texture are important characteristics that help identify it in the outdoor environment. Vision-based Smoke detection algorithms can be broadly classified into three categories: smoke classification, segmentation, and bounding box estimation. This paper presents a comprehensive survey of existing techniques on smoke detection in the outdoor environment using image and video analysis. To perform the survey, initially 271 articles were collected from different sources like Google Scholar, Science Direct, IEEE Xplore, SpringerLink, Wiley and ACM Digital Library using the keyword search. Based on their focus on the vision-based solutions for the outdoor environment, 126 articles were identified as relevant to the present survey. Starting from the initial IP approaches that are frequently referred in the literature, machine learning and deep learning approaches have also been reviewed for each type of smoke detection. Performance of algorithms, datasets used in the research, evaluation metrics, challenges and future directions of research are also discussed.}
}
@article{KUMAR2024108977,
title = {Wildfire and smoke early detection for drone applications: A light-weight deep learning approach},
journal = {Engineering Applications of Artificial Intelligence},
volume = {136},
pages = {108977},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108977},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624011357},
author = {Abhinav Kumar and Adolfo Perrusquía and Saba Al-Rubaye and Weisi Guo},
keywords = {Deeplabv3+, Mobile vision transformers, Mobilenet, Wildfire and smoke detection, Segmentation},
abstract = {Drones have become a crucial element in current wildfire and smoke detection applications. Several deep learning architectures have been developed to detect fire and smoke using either colour-based methodologies or semantic segmentation techniques with impressive results. However, the computational demands of these models reduce their usability on memory-restricted devices such as drones. To overcome this memory constraint whilst maintaining the high detection capabilities of deep learning models, this paper proposes two lightweight architectures for fire and smoke detection in forest environments. The approaches use the Deeplabv3+ architecture for image segmentation as baseline. The novelty lies in the incorporation of vision transformers and a lightweight convolutional neural network architecture that heavily reduces the model complexity, whilst maintaining state-of-the-art performance. Two datasets for fire and smoke segmentation, based on the Corsican, FLAME, SMOKE5K, and AI-For-Mankind datasets, are created to cover different real-world scenarios of wildfire to produce models with better detection capabilities. Experiments are conducted to show the benefits of the proposed approach and its relevance in current drone-based wildfire detection applications.}
}
@article{KANG2023113814,
title = {Toward an adaptable deep-learning model for satellite-based wildfire monitoring with consideration of environmental conditions},
journal = {Remote Sensing of Environment},
volume = {298},
pages = {113814},
year = {2023},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2023.113814},
url = {https://www.sciencedirect.com/science/article/pii/S0034425723003656},
author = {Yoojin Kang and Taejun Sung and Jungho Im},
keywords = {Active fire detection, Convolutional neural network, Robust to environmental changes, Geostationary satellite, Numerical and satellite data fusion},
abstract = {As the majority of active fire detection algorithms have been developed for worldwide applications using only satellite data without considering observing conditions and environmental factors, their performance varies regionally. This study investigates the viability of an adaptable active fire detection model that is applicable to diverse environmental and observing conditions by fusing numerical model data and satellite images. The model was developed for various land cover and climate types using commonly utilized brightness temperature-related variables (key variables) and supporting variables (sub-variables), including solar zenith angle, satellite zenith angle (SAZ), relative humidity (RH), and skin temperature. A dual-module (DM) convolutional neural network (CNN) structure was adopted to consider the different properties of key variables and sub-variables, and a control without sub-variables was used to assess the impact of observing and environmental variables. The proposed model was further evaluated using existing polar-orbiting and geostationary satellite-based active fire products. The recall and precision of the control model were 0.80 and 0.98, respectively, and the standard deviation of recall for the five focus sites was 0.140. However, the DM CNN model was notable for its higher recall and robustness compared to the control model (recall of 0.84, precision of 0.97, and standard deviation of recall of 0.126). High RH and SAZ, and the day-night transition period contributed to the poor performance of the control model which was mitigated by the DM CNN model. In particular, the use of RH improved the recall of the model, and SAZ contributed to the reduction of performance variation. Our model also outperformed the two geostationary satellite-based active fire products in terms of detection capacity, resulting in a spatial distribution of active fires similar to that of polar-orbiting satellite-based active fire products.}
}
@article{DIXON2023113842,
title = {Satellite detection of canopy-scale tree mortality and survival from California wildfires with spatio-temporal deep learning},
journal = {Remote Sensing of Environment},
volume = {298},
pages = {113842},
year = {2023},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2023.113842},
url = {https://www.sciencedirect.com/science/article/pii/S0034425723003930},
author = {Dan J. Dixon and Yunzhe Zhu and Christopher F. Brown and Yufang Jin},
keywords = {Tree mortality, CubeSat, Canopy-scale, Deep learning, Fire severity, Wildfire, PlanetScope, Convolutional neural network},
abstract = {Mapping forest disturbances is paramount to carbon monitoring, estimating environmental drivers, and developing strategies to enhance forest resilience. Existing forest change products from Landsat and Sentinel-2 have improved our understanding of large-scale disturbance patterns; however, their relatively coarse spatial resolution (10 to 30-m) leads to the use of mixed pixels which constrains their application for detecting heterogeneous survival or mortality outcomes occurring at the level of individual trees or canopies. PlanetScope multispectral imagery at 3-m and near-daily frequency offers new capabilities to detect and monitor diverse tree mortality patterns following disturbance across landscapes. This research proposes a framework to detect canopy-scale (3 × 3-m) tree/shrub mortality and survival using PlanetScope monthly time series. A 3D Spatio-Temporal Convolutional Neural Network (ST-CNN) deep learning model was designed to fully utilize the spatial context and the temporal change unique to canopy survival and mortality from the PlanetScope time series. As a crucial component for training robust and scalable deep learning models, a large set of labels was collected via a semi-automatic workflow by combining pre-disturbance lidar crown segmentation and post-disturbance aerial imagery interpretation. We applied the framework to detect canopy-scale mortality and survival following 15 large wildfires in California from 2018–2021. We sampled 1,176 384 × 384-m scenes from burned areas with pre-fire aerial lidar, containing >1.8M tree and shrub canopy polygons labeled as dead or alive following wildfire. Evaluated with an independent testing dataset, the optimized ST-CNN model detects heterogeneous patterns representing survival and mortality outcomes at 3-m resolution which accurately align with observed/labeled data. Tree mortality detection accuracy was high and stable in the Sierra Nevada and North Coast Mountains ecoregions (user’s = 83%–86%; producer’s = 81%–82%), but decreased slightly within the sparser Central Foothills and South Coast and Mountains (user’s = 77%–81%; producer’s = 58%–61%) often due to confusion between shrub and tree mortality. Producer’s accuracy of tree mortality and survival increased with canopy height and remained stable (>75%) on canopies taller than 11-m. Further, a sensitivity analysis demonstrates the performance benefits of using spatial and/or temporal convolutions in the ST-CNN architecture for model prediction. Lastly, we demonstrate the scalability of the ST-CNN for regional-scale application on all large 2020 wildfires in California (∼1.6 Mha burn area). The wall-to-wall post-fire maps showed an overall 3-m tree mortality rate of 58.8%, ranging from 32% to 94% among individual fires. The trained ST-CNN provides an ecologically detailed estimation of the pre-disturbance forest composition (trees, shrubs, non-woody) and their outcomes (survival or mortality) post-disturbance. These data will improve higher resolution monitoring and assessment of forest disturbance impacts, allow for better understanding of forest vulnerability, and support forest management strategies and actions.}
}
@article{ROY2024100142,
title = {Multi-resolution monitoring of the 2023 maui wildfires, implications and needs for satellite-based wildfire disaster monitoring},
journal = {Science of Remote Sensing},
volume = {10},
pages = {100142},
year = {2024},
issn = {2666-0172},
doi = {https://doi.org/10.1016/j.srs.2024.100142},
url = {https://www.sciencedirect.com/science/article/pii/S2666017224000269},
author = {David P. Roy and Hugo {De Lemos} and Haiyan Huang and Louis Giglio and Rasmus Houborg and Tomoaki Miura},
keywords = {Burned area mapping, Active fire detection, Fire radiative power (FRP), Hawaii, Wildfire disasters, MODIS, VIIRS, PlanetScope, Future needs},
abstract = {The August 2023 wildfires over the island of Maui, Hawaii were one of the deadliest U.S. wildfire incidents on record with 100 deaths and an estimated U.S. $5.5 billion cost. This study documents the incidence, extent, and characteristics of the 2023 Maui wildfires using multi-resolution global satellite fire products, and in so doing demonstrates their utility and limitations for detailed fire monitoring, and highlights outstanding satellite fire observation needs for wildfire monitoring. The NASA 500 m Moderate Resolution Imaging Spectroradiometer (MODIS) burned area product is compared with PlanetScope 3 m burned areas that were mapped using a published deep learning algorithm. In addition, all the August 2023 active fire detections provided by MODIS on the Terra and Aqua satellites and by the Visible Infrared Imaging Radiometer Suite (VIIRS) on the S-NPP and NOAA-20 satellites are used to investigate the geographic and temporal occurrence of the fires and their incidence relative to the 3 m mapped burned areas. The geographic and diurnal variation on the fire radiative power (FRP), available with the active fire detections, is presented to examine how energetically the fires were burning. The analysis is undertaken for all of Maui and for the town of Lahaina that was the major population center that burned. Satellite active fires were first detected August 8th, 2023 in the early morning (1:45 onwards) on the western slopes of Mt. Haleakalā and were last detected August 10th in the early morning (at 2:46) over Lahaina and on the western slopes of Mt. Haleakalā. The FRP available with the VIIRS satellite active fire detections indicate that the fires burned less intensely from the beginning to the end of this three day period, the nighttime fires generally burned more intensely than the daytime fires, and the most intensely burning fires occurred over Lahaina likely due to the high fuel load in the buildings compared to the vegetation that burned elsewhere. The MODIS 500 m burned area product was too coarse to map most of the 18 burned areas that were mapped unambiguously at 3 m resolution with PlanetScope and covered 29.60 km2, equivalent to about 1.6% of Maui. This study highlights the limitations of systematically derived satellite fire products for assessment before, during and after wildfire disaster events such as those experienced over Maui. The needs for future fire monitoring of wildfire disaster events, and the recommendation for a fire monitoring satellite constellation, are discussed.}
}
@article{BOROUJENI2024121962,
title = {IC-GAN: An Improved Conditional Generative Adversarial Network for RGB-to-IR image translation with applications to forest fire monitoring},
journal = {Expert Systems with Applications},
volume = {238},
pages = {121962},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121962},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423024648},
author = {Sayed Pedram Haeri Boroujeni and Abolfazl Razi},
keywords = {Deep learning, Unmanned aerial vehicle (UAV), Wildfire monitoring, Generative Adversarial Network (GAN), Image translation, Thermal images},
abstract = {This paper introduces a novel Deep Learning (DL) architecture for inferring temperature information from aerial true-color RGB images by transforming them into Infrared Radiation (IR) domain. This work is motivated by a few facts. First, off-the-shelf contemporary drones are typically equipped only with regular cameras. Second, IR heat-mapping cameras are costly and heavy for payload-limited drones. Third, additional communication channels and power supply would be needed when including IR cameras. Finally, IR cameras provide lower resolution and shorter distance ranges than RGB cameras. Therefore, learning-based translation of aerial IR recordings to RGB images can be extremely useful not only for new tests but also for offline processing of the currently available forest fire datasets with RGB images. We offer an Improved Conditional-Generative Adversarial Network (IC-GAN), where matched IR images are used as a condition to guide the translation process by the generator. The U-Net-based generator is concatenated with a mapper module to transform the output into a stack of diverse color spaces with learnable parameters. To avoid the unnecessary penalization of pixel-level disparities and achieve structural similarity, we include clustering alignment to the loss function. The proposed framework is compared against several state-of-the-art methods, including U-Net, Efficient U-Net, GAN, and Conditional-GAN from both subjective (human perception) and objective evaluation perspectives. The results support our method’s efficacy, demonstrating a significant improvement of around 6% in PSNR, 15% in UQI, 9% in SSIM, and 23% in IoU metrics.}
}
@article{ZHOU2024100076,
title = {Colour guided ground-to-UAV fire segmentation},
journal = {ISPRS Open Journal of Photogrammetry and Remote Sensing},
volume = {14},
pages = {100076},
year = {2024},
issn = {2667-3932},
doi = {https://doi.org/10.1016/j.ophoto.2024.100076},
url = {https://www.sciencedirect.com/science/article/pii/S2667393224000206},
author = {Rui Zhou and Tardi Tjahjadi},
keywords = {Ground-to-UAV, Domain adaptation, Colour features, Class mixing, UAV-based fire monitoring, Fire segmentation},
abstract = {Leveraging ground-annotated data for scene analysis on unmanned aerial vehicles (UAVs) can lead to valuable real-world applications. However, existing unsupervised domain adaptive (UDA) methods primarily focus on domain confusion, which raises conflicts among training data if there is a huge domain shift caused by variations in observation perspectives or locations. To illustrate this problem, we present a ground-to-UAV fire segmentation method as a novel benchmark to verify typical UDA methods, and propose an effective framework, Colour-Mix, to boost the performance of the segmentation method equivalent to the fully supervised level. First, we identify domain-invariant fire features by deriving fire-discriminating components (u*VS) defined in colour spaces Lu*v*, YUV, and HSV. Notably, we devise criteria to combine components that are beneficial for integrating colour signals into deep-learning training, thus significantly improving the generalisation abilities of the framework without resorting to UDA techniques. Second, we perform class-specific mixing to eliminate irrelevant background content on the ground scenario and enrich annotated fire samples for the UAV imagery. Third, we propose to disentangle the feature encoding for different domains and use class-specific mixing as robust training signals for the target domain. The framework is validated on the drone-captured dataset, Flame, by using the combined ground-level source datasets, Street Fire and Corsica Wildfires. The code is available at https://github.com/Rui-Zhou-2/Colour-Mix.}
}
@article{WEN2024109078,
title = {Detecting rice straw burning based on infrared and visible information fusion with UAV remote sensing},
journal = {Computers and Electronics in Agriculture},
volume = {222},
pages = {109078},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109078},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924004691},
author = {Hao Wen and Xikun Hu and Ping Zhong},
keywords = {Agricultural sustainability, Multisensor information fusion, Straw burning, Machine learning architectures, UAV remote sensing},
abstract = {Burning agricultural straw after reaping is a typical farming approach for quicker crop rotation but compromises the sustainability of cultivation practices. Unmanned aerial vehicle (UAV) remote sensing techniques are regarded as a feasible coping strategy to the sustainability dilemmas confronted in the domain. In this paper, we evaluate current multisensor information fusion technologies including both traditional methods and deep learning approaches for remote monitoring in real agricultural scenes and investigate their applicability for detecting burning rice straw. To this end, we collect StrawBurning, a real-world dataset containing fully paired infrared and visible images through mapping ground scenes based on UAV remote sensing. Furthermore, we propose a novel multiscale contrast adaptation (MCA) method for efficient multisensor image fusion and accurate straw burning detection in real farmland scenarios. The MCA remarkably enhanced the detection performance of YOLOv5 on the StrawBurning data by approximately 3%, 2%, and 5%, in terms of recall, mAP@0.5 and mAP@0.5:0.95, respectively. Therefore, our proposed method is demonstrated to obtain the superior performance compared to the single-modal information and other advanced multisensor information fusion methods. Experimental results indicate that neural network models utilizing the proposed multisensor fusion data showed the potential to accurately detect burning rice straw.}
}
@article{WARD20231,
title = {Conceptual optimization of remotely piloted amphibious aircraft for wildfire air attack},
journal = {Drone Systems and Applications},
volume = {11},
pages = {1-16},
year = {2023},
issn = {2564-4939},
doi = {https://doi.org/10.1139/dsa-2022-0051},
url = {https://www.sciencedirect.com/science/article/pii/S2564493923000152},
author = {Ryan Ward and Brett Readman and Brennan O’Yeung and W. Schuyler Hinman},
keywords = {drone design, multidisciplinary optimization, aircraft conceptual design, genetic algorithm},
abstract = {In this study, a methodology for the high-level conceptual design, optimization, and evaluation of amphibious remotely piloted and autonomous fixed-wing aircraft to support wildfire air attack strategies is presented. Of particular interest are questions of scale, water source utilization, and optimization of high-level aircraft parameters in a regional context. The Canadian province of British Columbia is used as a case study due to the relevance of wildfire control in that region. The present strategy incorporates a detailed analysis of available water bodies, tanker base locations, and their distance from historical wildfire locations and explores how these regionally specific details impact optimal aircraft design parameters. Results are obtained for optimal lake size as well as the primary design characteristics of the corresponding optimal aircraft. Two filling strategies are evaluated, namely, a “stop-and-go” strategy and a traditional skimming strategy. The results indicate the potential of fleets of optimized aircraft to supply high flow rates while capitalizing on the established benefits of using remotely piloted and autonomous systems. It is hoped this work will encourage future study into improved models and the further development of drone technology for this application, including necessary beyond visual line-of-sight technology and infrastructure.}
}
@article{SALEH2024e23127,
title = {Forest fire surveillance systems: A review of deep learning methods},
journal = {Heliyon},
volume = {10},
number = {1},
pages = {e23127},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e23127},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023103355},
author = {Azlan Saleh and Mohd Asyraf Zulkifley and Hazimah Haspi Harun and Francis Gaudreault and Ian Davison and Martin Spraggon},
keywords = {Forest preservation, Forest fire, Artificial intelligence and deep learning},
abstract = {This review aims to critically examine the existing state-of-the-art forest fire detection systems that are based on deep learning methods. In general, forest fire incidences bring significant negative impact to the economy, environment, and society. One of the crucial mitigation actions that needs to be readied is an effective forest fire detection system that are able to automatically notify the relevant parties on the incidence of forest fire as early as possible. This review paper has examined in details 37 research articles that have implemented deep learning (DL) model for forest fire detection, which were published between January 2018 and 2023. In this paper, in depth analysis has been performed to identify the quantity and type of data that includes images and video datasets, as well as data augmentation methods and the deep model architecture. This paper is structured into five subsections, each of which focuses on a specific application of deep learning (DL) in the context of forest fire detection. These subsections include 1) classification, 2) detection, 3) detection and classification, 4) segmentation, and 5) segmentation and classification. To compare the model’s performance, the methods were evaluated using comprehensive metrics like accuracy, mean average precision (mAP), F1-Score, mean pixel accuracy (MPA), etc. From the findings, of the usage of DL models for forest fire surveillance systems have yielded favourable outcomes, whereby the majority of studies managed to achieve accuracy rates that exceeds 90%. To further enhance the efficacy of these models, future research can explore the optimal fine-tuning of the hyper-parameters, integrate various satellite data, implement generative data augmentation techniques, and refine the DL model architecture. In conclusion, this paper highlights the potential of deep learning methods in enhancing forest fire detection that is crucial for forest fire management and mitigation.}
}
@article{BRANCO2023110091,
title = {The use of machine learning in species threats and conservation analysis},
journal = {Biological Conservation},
volume = {283},
pages = {110091},
year = {2023},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2023.110091},
url = {https://www.sciencedirect.com/science/article/pii/S0006320723001921},
author = {Vasco Veiga Branco and Luís Correia and Pedro Cardoso},
keywords = {Artificial intelligence, Conservation Biology, Species conservation, Extinction risk, Systematic review},
abstract = {The concepts and methodologies of machine learning are increasingly used to create semi-autonomous programmes capable of adapting to a multitude of problems and decision-making scenarios. With its potential in big data analysis, machine learning is particularly useful for tackling global conservation problems that often involve vast amounts of data and complex interactions between variables. In this systematic review, we summarise the use of machine learning methods in the study of species threats and conservation measures, and their emergent trends. Maximum entropy, Bayesian (regression or classification models) and ensemble methods (tree-based models, either bagging or boosting) have gained wide popularity in the past years and are now commonly used for multiple problems. Their relevance to modern conservation issues (and associated data types), their relatively simple implementation, and availability in a variety of software packages are the most likely factors to explain their popularity. Neural networks, decision trees, support-vector machines and evolutionary algorithms have been used in more specific situations, with some model applications showing promise in dealing with increasingly complex data and scenarios.}
}
@incollection{OLAWALE202093,
title = {Chapter Six - AI simulations and programming environments for drones: an overview},
editor = {Fadi Al-Turjman},
booktitle = {Drones in Smart-Cities},
publisher = {Elsevier},
pages = {93-106},
year = {2020},
isbn = {978-0-12-819972-5},
doi = {https://doi.org/10.1016/B978-0-12-819972-5.00006-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128199725000069},
author = {Oluwaseun Priscilla Olawale and Kamil Dimililer and Fadi Al-Turjman},
keywords = {drones, artificial intelligence, simulation, models, system, UAVs},
abstract = {Simulators are generally used in the field of robotics, medical applications to virtually understand a system. They help in economizing cost of the actual implementation of robotic systems. The main reason for the simulation of systems (such as surgical robots, etc.) with artificial intelligence (AI) algorithms is to enable them learn how to perform tasks without the aid of any human intervention. Artificial Intelligence is “Compulsory” as it plays a very vital role in the simulation of drones. Without it, deploying and maintaining the actual drone system will be extremely expensive. It will also be impossible to program them carry out specific tasks, especially those that endanger human life. We generally present an overview of AI Simulations and Programming Environments for Drones in our research. First, we briefly analyze the use of simulators with AI, particularly on drones otherwise known as quadcopter or unmanned aerial vehicles. Second, we compare simulation environments and programming languages used in the development of their system application.}
}
@article{NHANGUMBE2023101015,
title = {Supervised and unsupervised machine learning approaches using Sentinel data for flood mapping and damage assessment in Mozambique},
journal = {Remote Sensing Applications: Society and Environment},
volume = {32},
pages = {101015},
year = {2023},
issn = {2352-9385},
doi = {https://doi.org/10.1016/j.rsase.2023.101015},
url = {https://www.sciencedirect.com/science/article/pii/S2352938523000976},
author = {Manuel Nhangumbe and Andrea Nascetti and Stefanos Georganos and Yifang Ban},
keywords = {Sentinel-1 and Sentinel-2, DrivenData dataset, Flood mapping, Classification, Damage assessment},
abstract = {Natural hazards, such as flooding, have been negatively impacting developed and emerging economies alike. The effects of floods are more prominent in countries of the Global South, where large parts of the population and infrastructure are insufficiently protected from natural hazards. From this scope, a lot of effort is required to mitigate these impacts by continuously providing new and more reliable tools to aid in mitigation and preparedness, during or after a flood event. Flood mapping followed by damage assessment plays an important role in all these stages. In this work we investigate a new dataset provided by DrivenData Labs based on Sentinel-1 (S1) imagery (VH, VV imagery and labels) to help map floods in the city of Beira in Mozambique. Exploiting Google Earth Engine (GEE), we deployed supervised and unsupervised machine learning (ML) methods on a dataset comprising imagery from 13 countries worldwide. We first mapped the floods country-by-country including Mozambique. This first part was helpful to understand the sensitivity of each method when applied to data from different regions and with different polarizations. We then trained the supervised model globally (in all 13 countries) and used it to predict floods in Beira. To assess the accuracy of the experiments we used the intersection over the union (IoU) metric, results of which we compared with the benchmark IoU achieved by the winner in the DrivenData competition for flood mapping in 2021. The implementation of unsupervised and supervised ML using VH and VV+VH produced satisfactory results, and showed to be better than using VV imagery; in Cambodia and Bolivia with VH polarization yielded IoUs values ranging from 0.819 to 0.856 which is above the benchmark (0.8094). The predictions in Beira using VH imagery yielded IoU of 0.568, which is a reasonable outcome. The proposed approach is a reliable alternative for flood mapping, especially in Mozambique due to its low cost and time effectiveness as even with unsupervised approaches, relatively high-quality results are yielded in near real-time. Finally, we used Sentinel-2 (S2) imagery for a land cover classification to perform damage assessment in Beira and integrated population data from Beira to enhance the quality the results. The results show that 20% of agricultural area and about 10% of built up area were flooded. Flooded built up area includes highly populated neighborhoods such as Chaimite and Ponta Gea that are located in the center of the city.}
}
@article{REJEB2021100434,
title = {Humanitarian Drones: A Review and Research Agenda},
journal = {Internet of Things},
volume = {16},
pages = {100434},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100434},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000780},
author = {Abderahman Rejeb and Karim Rejeb and Steve Simske and Horst Treiblmaier},
keywords = {drones, unmanned aerial vehicles, unmanned aerial system, humanitarian logistics, literature review, research agenda},
abstract = {This study investigates the capabilities, performance outcomes, and barriers of drones applied to humanitarian logistics (HL). A systematic literature review was conducted to synthesize prior research on drones and cumulatively identify current knowledge gaps which require further investigation. In order to identify the relevant literature on the topic, a rigorous research protocol was applied for the retrieval and selection processes. In total, 142 publications fulfilled the selection criteria and were thoroughly analyzed. The findings of this review paper summarize the capabilities, barriers and performance outcomes of humanitarian drones applied to logistics operations, management, and governance in a comprehensive framework. More specifically, three important capabilities (i.e., transportation and delivery; surveying and monitoring; communication and integration), three performance outcomes (i.e., flexibility and responsiveness; cost reduction; sustainability) and adoption barriers in three areas (i.e., technology; organization; environment) were identified. Based on these findings, future research directions are derived for the capabilities of humanitarian drones, their performance outcomes, and their respective barriers. This study analyzes potential applications of drones in the humanitarian field and presents a comprehensive agenda that structures and guides further research on the topic.}
}
@article{LABENSKI2022102799,
title = {Classifying surface fuel types based on forest stand photographs and satellite time series using deep learning},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {109},
pages = {102799},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102799},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222000012},
author = {Pia Labenski and Michael Ewald and Sebastian Schmidtlein and Fabian Ewald Fassnacht},
keywords = {Fuel types, Deep neural networks, Image classification, Convolutional neural networks, Long short-term memory, Sentinel-2},
abstract = {With the increasing threat of wildfires globally, improving the availability of accurate, spatially explicit fuel type information is critical for fire behavior predictions that can support management decisions to mitigate fire hazards. Since mapping surface fuel types using airborne or spaceborne sensors relies on ground truth data from laborious field assessments, here we propose a novel proximate sensing-based approach for classifying surface fuel types from in-forest RGB photographs using convolutional neural networks (CNNs). We test different configurations of deep learning models that integrate photographs of the forest stand and the forest floor as well as time series of multispectral satellite data from Sentinel-2 using long short-term memory (LSTM), and compare their performance in classifying understory and litter fuel types of Central European forests. We also investigate how ensemble approaches based on majority voting can help to improve classification results. We found that understory fuel types were classified with highest accuracy after cross-validation (0.78) using a combination of horizontal stand photos and forest floor photos. This accuracy was further improved by post-classification decision fusion of model predictions on multiple photographs of a forest stand and by considering the model’s confidence in its predictions (0.85). Litter fuel type classification based on forest photographs resulted in lower overall accuracy (0.60), but using model ensemble predictions on both photographs and Sentinel-2 time series significantly improved the results (0.72). We found that the accuracy of our models was mostly limited by naturally smooth transitions between the defined fuel type classes and the co-occurrence of multiple fuel types in a photograph. This study shows that deep learning methods can provide an efficient means to assess fuel types from GNSS-located photos of forest stands as a basis for generating and validating fuel type and finally fire risk maps. The necessary data can be readily collected by forest managers or citizen scientists.}
}
@article{ALLQUBAYDHI2024100614,
title = {Deep learning for unmanned aerial vehicles detection: A review},
journal = {Computer Science Review},
volume = {51},
pages = {100614},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2023.100614},
url = {https://www.sciencedirect.com/science/article/pii/S1574013723000813},
author = {Nader Al-lQubaydhi and Abdulrahman Alenezi and Turki Alanazi and Abdulrahman Senyor and Naif Alanezi and Bandar Alotaibi and Munif Alotaibi and Abdul Razaque and Salim Hariri},
keywords = {Drone detection, Deep learning, Convolutional neural network, Recurrent neural network, Unmanned aerial vehicle},
abstract = {As a new type of aerial robotics, drones are easy to use and inexpensive, which has facilitated their acquisition by individuals and organizations. This unequivocal and widespread presence of amateur drones may cause many dangers, such as privacy breaches by reaching sensitive locations of authorities and individuals. In this paper, we summarize the performance-affecting factors and major obstacles to drone use and provide a brief background of deep learning. Then, we summarize the types of UAVs and the related unethical behaviors, safety, privacy, and cybersecurity concerns. Then, we present a comprehensive literature review of current drone detection methods based on deep learning. This area of research has arisen in the last two decades because of the rapid advancement of commercial and recreational drones and their combined risk to the safety of airspace. Various deep learning algorithms and their frameworks with respect to the techniques used to detect drones and their areas of applications are also discussed. Drone detection techniques are classified into four categories: visual, radar, acoustics, and radio frequency-based approaches. The findings of this study prove that deep learning-based detection and classification of drones looks promising despite several challenges. Finally, we provide some recommendations to meet future expectations.}
}
@article{BOROUJENI2024102369,
title = {A comprehensive survey of research towards AI-enabled unmanned aerial systems in pre-, active-, and post-wildfire management},
journal = {Information Fusion},
volume = {108},
pages = {102369},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102369},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524001477},
author = {Sayed Pedram Haeri Boroujeni and Abolfazl Razi and Sahand Khoshdel and Fatemeh Afghah and Janice L. Coen and Leo O’Neill and Peter Fule and Adam Watts and Nick-Marios T. Kokolakis and Kyriakos G. Vamvoudakis},
keywords = {Wildfire management, Artificial intelligence (AI), Unmanned aerial vehicle (UAV), Machine learning, Deep learning (DL), Reinforcement learning (RL), Computer vision},
abstract = {Wildfires have emerged as one of the most destructive natural disasters worldwide, causing catastrophic losses. These losses have underscored the urgent need to improve public knowledge and advance existing techniques in wildfire management. Recently, the use of Artificial Intelligence (AI) in wildfires, propelled by the integration of Unmanned Aerial Vehicles (UAVs) and deep learning models, has created an unprecedented momentum to implement and develop more effective wildfire management. Although existing survey papers have explored learning-based approaches in wildfire, drone use in disaster management, and wildfire risk assessment, a comprehensive review emphasizing the application of AI-enabled UAV systems and investigating the role of learning-based methods throughout the overall workflow of multi-stage wildfire management, including pre-fire (e.g., vision-based vegetation fuel measurement), active-fire (e.g., fire growth modeling), and post-fire tasks (e.g., evacuation planning) is notably lacking. This survey synthesizes and integrates state-of-the-science reviews and research at the nexus of wildfire observations and modeling, AI, and UAVs — topics at the forefront of advances in wildfire management, elucidating the role of AI in performing monitoring and actuation tasks from pre-fire, through the active-fire stage, to post-fire management. To this aim, we provide an extensive analysis of the existing remote sensing systems with a particular focus on the UAV advancements, device specifications, and sensor technologies relevant to wildfire management. We also examine the pre-fire and post-fire management approaches, including fuel monitoring, prevention strategies, as well as evacuation planning, damage assessment, and operation strategies. Additionally, we review and summarize a wide range of computer vision techniques in active-fire management, with an emphasis on Machine Learning (ML), Reinforcement Learning (RL), and Deep Learning (DL) algorithms for wildfire classification, segmentation, detection, and monitoring tasks. Ultimately, we underscore the substantial advancement in wildfire modeling through the integration of cutting-edge AI techniques and UAV-based data, providing novel insights and enhanced predictive capabilities to understand dynamic wildfire behavior.}
}
@article{CHUNG2020105004,
title = {Optimization for drone and drone-truck combined operations: A review of the state of the art and future directions},
journal = {Computers & Operations Research},
volume = {123},
pages = {105004},
year = {2020},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2020.105004},
url = {https://www.sciencedirect.com/science/article/pii/S0305054820301210},
author = {Sung Hoon Chung and Bhawesh Sah and Jinkun Lee},
keywords = {Drone, Drone-truck combined operations, Survey article, Unmanned aerial vehicle, Transportation planning, Optimization},
abstract = {This paper surveys the state-of-the-art optimization approaches in the civil application of drone operations (DO) and drone-truck combined operations (DTCO) including construction/infrastructure, agriculture, transportation/logistics, security/disaster management, entertainment/media, etc. In particular, this paper reviews ongoing research on various optimization issues related to DO and DTCO including mathematical models, solution methods, synchronization between a drone and a truck, and barriers in implementing DO and DTCO. First, the paper introduces DO and DTCO and their applications, and explores some previous works including survey papers. In addition, this paper surveys the state of the art of DO and DTCO studies and discusses the research gaps in the literature. Furthermore, the detailed review of DTCO models and solution methods are reviewed. Finally, future research directions are discussed.}
}
@article{QI202249,
title = {Comparing tree attributes derived from quantitative structure models based on drone and mobile laser scanning point clouds across varying canopy cover conditions},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {192},
pages = {49-65},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2022.07.021},
url = {https://www.sciencedirect.com/science/article/pii/S0924271622002015},
author = {Yangqian Qi and Nicholas C. Coops and Lori D. Daniels and Christopher R. Butson},
keywords = {Tree attributes, Laser scanning, Drone, SLAM, Canopy cover, Quantitative structure model (QSM)},
abstract = {Complex canopy cover conditions often challenge the accurate measurement of many individual tree attributes that are pivotal to the sustainable management of forest resources. Advances in drone laser scanning (DLS) and mobile laser scanning (MLS) have enabled the acquisition of high-density point clouds with the potential to better resolve detailed tree structures. Yet, the quality of DLS and MLS data can be limited by occlusions and environmental complexities. To quantify the impacts of canopy cover on the tree attribute estimation, this study investigated the utility of DLS and MLS data both individually and combined. Considering the scanning characteristics, we examined direct fusion and a new strategy using a relative weighting scheme based on the probability density of vertical point distribution. We compared the accuracy of seven tree attributes derived from quantitative structure models (QSMs) based on (1) DLS, (2) MLS, (3) fused, and (4) weighted point clouds under low, moderate, and high canopy cover levels. We found that the weighted data improved the modelling efficiency of QSMs by ∼ 20% on average, compared to fused and MLS data. Across canopy cover levels, the fused and weighted data achieved comparable results and outperformed DLS/MLS data in estimating tree attributes. Specifically, diameter at breast height and crown base height were accurately extracted from the fused, weighted, and MLS data under low canopy cover with the concordance correlation coefficient (CCC) > 0.80. As canopy cover increased, they were best estimated using the fused data (CCC > 0.90, RRMSE < 22%). Height was accurate regardless of canopy cover, which was independent of data collection platforms (CCC > 0.80, RRMSE < 16%). The crown diameter was also well estimated by fused, weighted, and MLS data across canopy cover levels (CCC > 0.82, RRMSE < 19%). The total, stem, and branch volumes could be best modelled by the fused data with increasing canopy cover. Overall, the fusion of DLS and MLS point clouds allowed the retrieval of comprehensive tree-level information. However, forestry practitioners still need to evaluate the trade-offs in selecting the most appropriate platform for laser scanning data based on their needs. Future studies should also enhance the modelling of trees with complex branching structures to strengthen the extraction of diverse attributes.}
}
@article{SHARMA2024107781,
title = {Object detection in power line infrastructure: A review of the challenges and solutions},
journal = {Engineering Applications of Artificial Intelligence},
volume = {130},
pages = {107781},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107781},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623019656},
author = {Pratibha Sharma and Sumeet Saurav and Sanjay Singh},
keywords = {Object detection, Power line inspection, Computer vision, Power line infrastructure monitoring},
abstract = {Lack of proper maintenance of power line infrastructures is one of the main reasons behind power shortages and major blackouts. Current inspection methods are human-dependent, which is time-consuming and expensive. Recent progress in Unmanned Aerial Vehicles (UAVs) and digital cameras enforces the use of UAVs for power line inspection, reducing the cost and time to a great extent. Deep learning methods have recently proved their efficacy in the automatic analysis of power line data; however, they suffer from numerous challenges. Unlike generic object detection, power line inspection does not have large datasets. The data collection of power line objects is challenging compared to data collection for generic objects. As deep learning methods are data-hungry, difficulty in collecting training data raises class imbalance problems. Also, the real-time inspection of power line components demands compute-efficient deep learning methods, which is also challenging because of the high computational requirements of the generic deep learning-based object detectors. Despite being researched for decades, no object detectors can eliminate the effect of diverse challenges on the performance of deep learning methods. With these considerations, this study thoroughly reviews the existing works in the literature and the methods and approaches adopted in power line inspection to overcome these challenges. We also provide the type of faults addressed in the literature with details on the methods employed for their analysis. Finally, we conclude the review by providing insights into future research directions in power line inspection.}
}
@article{DUHAYYIM20231355,
title = {Fusion-Based Deep Learning Model for Automated Forest Fire Detection},
journal = {Computers, Materials and Continua},
volume = {77},
number = {1},
pages = {1355-1371},
year = {2023},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.024198},
url = {https://www.sciencedirect.com/science/article/pii/S1546221823000875},
author = {Mesfer Al Duhayyim and Majdy M. Eltahir and Ola Abdelgney {Omer Ali} and Amani Abdulrahman Albraikan and Fahd N. Al-Wesabi and Anwer Mustafa Hilal and Manar Ahmed Hamza and Mohammed Rizwanullah},
keywords = {Environment monitoring, remote sensing, forest fire detection, deep learning, machine learning, fusion model},
abstract = {Earth resource and environmental monitoring are essential areas that can be used to investigate the environmental conditions and natural resources supporting sustainable policy development, regulatory measures, and their implementation elevating the environment. Large-scale forest fire is considered a major harmful hazard that affects climate change and life over the globe. Therefore, the early identification of forest fires using automated tools is essential to avoid the spread of fire to a large extent. Therefore, this paper focuses on the design of automated forest fire detection using a fusion-based deep learning (AFFD-FDL) model for environmental monitoring. The AFFD-FDL technique involves the design of an entropy-based fusion model for feature extraction. The combination of the handcrafted features using histogram of gradients (HOG) with deep features using SqueezeNet and Inception v3 models. Besides, an optimal extreme learning machine (ELM) based classifier is used to identify the existence of fire or not. In order to properly tune the parameters of the ELM model, the oppositional glowworm swarm optimization (OGSO) algorithm is employed and thereby improves the forest fire detection performance. A wide range of simulation analyses takes place on a benchmark dataset and the results are inspected under several aspects. The experimental results highlighted the betterment of the AFFD-FDL technique over the recent state of art techniques.}
}
@article{WANG2024123489,
title = {M4SFWD: A Multi-Faceted synthetic dataset for remote sensing forest wildfires detection},
journal = {Expert Systems with Applications},
volume = {248},
pages = {123489},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123489},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424003543},
author = {Guanbo Wang and Haiyan Li and Peng Li and Xun Lang and Yanling Feng and Zhaisehng Ding and Shidong Xie},
keywords = {Synthetic data, Forest wildfire detection, Multiscale object detection, Multi-faceted remote sensing data},
abstract = {Forest wildfires are one of the most catastrophic natural disasters, which poses a severe threat to both the ecosystem and human life. Therefore, it is imperative to implement technology to prevent and control forest wildfires. The combination of unmanned aerial vehicles (UAVs) and object detection algorithms provides a quick and accurate method to monitor large-scale forest areas. Nevertheless, most available datasets on forest wildfires comprise single-mode ground-fixed-angle pictures that inadequately represent the intricate terrain, high humidity, low visibility meteorological conditions, and multiscale light flux densities of forest wildfires. To address these limitations, we developed the Multiple scenarios, Multiple weather conditions, Multiple lighting levels and Multiple wildfire objects Synthetic Forest Wildfire Dataset (M4SFWD), which provides remote sensing data on forest fires across diverse terrain types, weather conditions, light flux densities as well as different numbers of wildfire objects. Researchers can employ this dataset to improve the efficacy of fire and smoke detection algorithms, promoting continuous forest monitoring. This paper presents a Multi-Faceted Synthetic Forest Wildfire Dataset based on Unreal Engine 5. We first constructed eight forest scenes with different terrains, weather conditions, and texture effects. We also simulated the light flux density at different times of the day by utilizing real-time ray tracing technology, which created realistic lighting and shadows. Secondly, we introduced a range of wildfire targets with varying scales and numbers into each scenario to enable multiple-angle shooting simulations from a UAV’s viewpoint. During evening hours and in foggy conditions, many objects resemble wildfires. To enhance the dataset’s precision and reliability for fire and smoke detection, 3,974 images were undergone pixel-level manual annotation using tools like labelImg. This annotation yielded 17,763 bounding boxes, which were subsequently statistically analyzed to ascertain their positions and proportions. Finally, we assessed the applicability of M4SFWD in single-stage, two-stage, and lightweight object detection algorithms by inputting the dataset into various algorithms with different parameter sizes. Based on the experimental results’ visualization, M4SFWD exhibited superior performance in scenarios with standard light flux density and large-scale wildfire objects. However, due to its complex contextual information and multiscale object features, false detections and missed detections occurred in other complex multi-faceted scenarios. Thus, optimizing the existing object detection algorithms will be necessary for future research. The dataset is available at: https://github.com/Philharmy-Wang/M4SFWD.}
}
@article{LAWRENCE2024100167,
title = {Using computer vision to classify, locate and segment fire behavior in UAS-captured images},
journal = {Science of Remote Sensing},
volume = {10},
pages = {100167},
year = {2024},
issn = {2666-0172},
doi = {https://doi.org/10.1016/j.srs.2024.100167},
url = {https://www.sciencedirect.com/science/article/pii/S2666017224000518},
author = {Brett L. Lawrence and Emerson {de Lemmus}},
keywords = {YOLO, Computer vision, Fire behavior, Fire detection, UAS},
abstract = {The widely adaptable capabilities of artificial intelligence, in particular deep learning and computer vision have led to significant research output regarding flame and smoke detection. The composition of flame and smoke, also described as fire behavior, can be considerably different depending on factors like weather, fuels, and the specific landscape fire is being observed on. The ability to detect definable classes of fire behavior using computer vision has not been explored and could be helpful given it often dictates how firefighters respond to fire situations. To test whether types of fire behavior could be reliably classified, we collected and labeled a unique unmanned aerial system (UAS) image dataset of fire behavior classifications to be trained and validated using You Only Look Once (YOLO) detection models. Our 960 labeled images were sourced from over 21 h of UAS video collected during prescribed fire operations covering a large region of Texas and Louisiana, United States. National Wildfire Coordinating Group (NWCG) fire behavior observations and descriptions served as a reference for determining fire behavior classes during labeling. YOLOv8 models were trained on NWCG Rank 1–3 fire behavior descriptions in grassland, shrubland, forested, and combined fire regimes within our study area. Models were first trained and validated on classifying isolated image objects of fire behavior, and then separately trained to locate and segment fire behavior classifications in UAS images. Models trained to classify isolated image objects of fire behavior consistently performed at a mAP of 0.808 or higher, with combined fire regimes producing the best results (mAP = 0.897). Most segmentation models performed relatively poorly, except for the forest regime model at a box (locate) and mask (segment) mAP of 0.59 and 0.611, respectively. Our results indicate that classifying fire behavior with computer vision is possible in different fire regimes and fuel models, whereas locating and segmenting fire behavior types around background information is relatively difficult. However, it may be a manageable task with enough data, and when models are developed for a specific fire regime. With an increasing number of destructive wildfires and new challenges confronting fire managers, identifying how new technologies can quickly assess wildfire situations can assist wildfire responder awareness. Our conclusion is that levels of abstraction deeper than just detection of smoke or flame are possible using computer vision and could make even more detailed aerial fire monitoring possible using a UAS.}
}
@article{DEEBAK2023109664,
title = {Intelligent drone-assisted robust lightweight multi-factor authentication for military zone surveillance in the 6G era},
journal = {Computer Networks},
volume = {225},
pages = {109664},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109664},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623001093},
author = {B.D. Deebak and Seong Oun Hwang},
keywords = {Unmanned aerial vehicle, B5G/6G, Aerial ad hoc network, Multi-factor authentication, Artificial intelligence, Security},
abstract = {In the diverse range of surveillance applications, large-scale deployment of next-generation communication technologies and the fast-growing development of unmanned aerial vehicles (UAVs) are envisioned as key innovations in the adoption of beyond-fifth generation (B5G) and 6G communication. Due to its self-reliance and versatility, a complex communication network can be formulated strategically to improve the application features of drone technology, including search-and-rescue, mission-critical services, and military surveillance. In recent times, technological advancements in hardware and software infrastructure have gained momentum toward seamless information interaction in aerial communication. Unfortunately, the recurrent process of user authentication causes severe communication instability in an unmanned aerial ad hoc network (UAANET) leading to some serious cyber threats, such as buffer overflow, denial of service, and spoofing. Therefore, building secure and reliable authentication is inevitable in order to protect drone-aided healthcare service environments. To protect aerial zones and improve security efficiency, this paper designs robust lightweight secure multi-factor authentication (RL-SMFA). The proposed RL-SMFA utilizes an AI-enabled, secure analytics phase to verify the genuineness of drone swarms for the ground control station. While protecting communication with drone vehicles, we also observe that power consumption by drones is reduced to a large extent. Using formal verification under a random oracle model, we show that the proposed RL-SMFA can functionally resist system vulnerabilities and constructively decrease the computation and communication costs of the UAANET. Lastly, the simulation study using ns3 shows that the proposed RL-SMFA achieves better performance efficiencies in terms of throughput rate, packet delivery ratio, and end-to-end delay than other state-of-the-art approaches to discovering a proper link establishment.}
}
@article{VONNONN2024105903,
title = {An open-source workflow for scaling burn severity metrics from drone to satellite to support post-fire watershed management},
journal = {Environmental Modelling & Software},
volume = {172},
pages = {105903},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105903},
url = {https://www.sciencedirect.com/science/article/pii/S136481522300289X},
author = {Joshua {von Nonn} and Miguel L. Villarreal and Leonhard Blesius and Jerry Davis and Skye Corbett},
keywords = {Uncrewed aerial systems, Burn severity, Wildfire, Free and open-source software, Remote sensing, Sentinel-2},
abstract = {Wildfires are increasing in size and severity across much of the western United States, exposing vulnerable wildland-urban interfaces to post-fire hazards. The Mediterranean chaparral region of Northern California contains many high sloping watersheds prone to hazardous post-fire flood events and identifying watersheds at high risk of soil loss and debris flows is a priority for post-fire response and management. Uncrewed Aerial Systems (UAS; aka drones) offer post-fire management teams the ability to quickly mobilize and survey burned areas with very high-resolution imagery (∼1 cm), facilitating emergency management and post-fire hazard assessment. However, adoption of this technology by hazard response teams may be hindered by complicated workflows for UAS data acquisition, image processing and analysis. We present an open-source workflow using mature Geographic Information Systems (GIS) software and Python packages in a Jupyter Notebook environment that guides users through classification of true-color UAS imagery to generate high resolution burn severity maps which can then be scaled across larger watersheds using Sentinel-2 normalized burn ratio (NBR) images. Soil burn severity classifications using a weighted brightness (WB) image and Char Index (CI) generated from UAS imagery were validated with in-situ data and random stratified points, resulting in the CI having the highest overall accuracy of 87.5%. CI also displayed a marginally stronger relationship over the WB with the post-fire Sentinel-2 NBR, R2 = 0.79 and R2 = 0.78 respectively. Our methods offer the unique opportunity to standardize GIS workflows, promoting replication through transparency, while improving the user's understanding of scientific GIS functionality.}
}
@article{KAMATH202334,
title = {Deep learning based object detection for resource constrained devices: Systematic review, future trends and challenges ahead},
journal = {Neurocomputing},
volume = {531},
pages = {34-60},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223001388},
author = {Vidya Kamath and A. Renuka},
keywords = {Object detection, Deep learning, Resource-constrained, Lightweight, Systematic literature review, Computer vision},
abstract = {Deep learning models are widely being employed for object detection due to their high performance. However, the majority of applications that require object detection are functioning on resource-constrained edge devices. In the present era, there is a need for deep learning-based object detectors that are lightweight and perform well on these constrained edge devices. Objective: The research aims to identify current trends in resource-constrained applications for deep learning-based object detectors in terms of the technique used to create the model, the type of input image involved, the type of device used, and the type of application addressed by the model. Method: To achieve the objective of our research, a systematic literature review was carried out that yielded 167 studies. The models or techniques employed in the studies were grouped to better understand the research problem at hand. This review carefully reports every decision and provides many visualizations of the final studies in order to draw clear conclusions. Conclusion: The conclusion discussed the gaps, possibilities, and future perspectives discovered throughout the research process, implying that this field of study has grown profoundly in the last decade.}
}
@incollection{CHATTERJEE2024393,
title = {Chapter Seventeen - Grid resilience against wildfire with machine learning: Machine learning based detection, localization and mitigation of the impact of forest fires on power grids},
editor = {Reza Arghandeh and Yuxun Zhou},
booktitle = {Big Data Application in Power Systems (Second Edition)},
publisher = {Elsevier Science},
edition = {Second Edition},
pages = {393-417},
year = {2024},
isbn = {978-0-443-21524-7},
doi = {https://doi.org/10.1016/B978-0-443-21524-7.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443215247000050},
author = {Paroma Chatterjee and Salah Uddin Kadir and Anurag Srivastava and Aron Laszka},
keywords = {Deep learning, Power system, Proactive control, Reinforcement learning, Resiliency, Wildfire},
abstract = {Decision-making by human operators, using system data obtained from bulk transmission systems, under adverse dynamic events should be supplemented by intelligent proactive control based on state-of-the-art machine learning (ML) algorithms. This chapter focuses on the integration of ML into transmission system operation during wildfires for resiliency-driven proactive control for load shedding, line switching, and resource allocation, considering the dynamics of the wildfire and failure propagation through the power grid to minimize impact on the system.}
}
@article{DARYNOVA2023100210,
title = {Data assimilation method for quantifying controlled methane releases using a drone and ground-sensors},
journal = {Atmospheric Environment: X},
volume = {17},
pages = {100210},
year = {2023},
issn = {2590-1621},
doi = {https://doi.org/10.1016/j.aeaoa.2023.100210},
url = {https://www.sciencedirect.com/science/article/pii/S2590162123000102},
author = {Zhuldyz Darynova and Benoit Blanco and Catherine Juery and Ludovic Donnat and Olivier Duclaux},
abstract = {This work assesses the particle filter data-assimilation technique for estimating the methane emission rate during CH4 controlled-release experiments conducted over 3–4 days in Fall 2020 and 2021. Several controlled methane releases took place on a 40 m × 50 m platform in France, called TADI (TotalEnergies Anomaly Detection Initiative). The leaks ranged from 0.01 to 5 g CH4 s-1 over 24–71 min. A methane-detecting drone and five ground-sensors recorded the methane concentration simultaneously. The accuracy of the air contaminant dispersion estimations, based on Gaussian model, is improved by applying a data assimilation method using a particle filter. Diffusion coefficients and release rate are considered as state parameters in the data-driven modeling. A particle filter is then applied to update these parameters during each computation step. We assessed various frameworks for assimilating air data in order to monitor CH4 emissions from industrial sites and infrastructures. For most releases, the assimilations consistently give precise rate estimates, whether considering fixed or mobile data and any of the particular assimilation setups. The average relative errors in the estimated CH4 release rates typically range from approximately 35%–84% for the 2020 campaign, and from 29% to 72% for the 2021 campaign. The inversion results using the stationary measurements have an average relative error of about 72%, while the use of drone measurements yields a more accurate emission rate estimate of around 51%. The hybrid approach, which simultaneously evaluated both drone and stationary measurements using a particle filter, achieved the highest coefficient of determination and the lowest relative error between the reported and model estimated flow rates (R2 = 0.97 and 29%, respectively).}
}
@article{RIBEIRO2023565,
title = {Burned area semantic segmentation: A novel dataset and evaluation using convolutional networks},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {202},
pages = {565-580},
year = {2023},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2023.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0924271623001831},
author = {Tiago F.R. Ribeiro and Fernando Silva and José Moreira and Rogério Luís de C. Costa},
keywords = {Forest fires, Burned area segmentation, UAV, Deep learning, Benchmark},
abstract = {Wildfires have significant impacts on the environment, society, and economy. Consequently, understanding its dynamics is crucial to evaluate such effects. Nonetheless, monitoring and measuring the burned area by traditional, non-automatic methods remains time-consuming and challenging. For several years, automatic semantic segmentation models have been used to describe natural phenomena, but deep learning models have recently achieved very competitive results. However, this new breed of models typically needs annotated datasets of significant dimensions. Nonetheless, datasets for real-time burnt area segmentation are often scarce. In this article, we create tools to support the benchmarking for testing and validating burned area segmentation models in a wildfire context. As such, we propose a new manually annotated dataset for segmentation of forest fire burned area based on a video captured by a UAV to train and evaluate semantic segmentation models. We suggest specific temporal consistency metrics to validate burned area polygons generated by the models in successive frames of non-annotated data. We also explore deep learning-based techniques and establish baselines, including IoU values superior to 95% on the test set.}
}
@article{PARK2022103052,
title = {Advanced wildfire detection using generative adversarial network-based augmented datasets and weakly supervised object localization},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {114},
pages = {103052},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.103052},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222002400},
author = {Minsoo Park and Dai Quoc Tran and Jinyeong Bak and Seunghee Park},
keywords = {Disaster response system, Decision support, Synthetic data, Weakly supervised object localization, Channel attention module},
abstract = {Owing to abnormal climate phenomena worldwide, forests are becoming dry and heat waves have started to occur, increasing the damage caused by wildfires. In addition to causing significant human and material damage, wildfires are also a major cause of critical pollutant emissions, in which fine dust generated by incomplete combustion pollutes the atmosphere, soil, and water. Early detection and monitoring are some of the main ways for minimizing wildfire damage, and a topic of research interest in various fields of artificial intelligence and computer vision. However, the lack of wildfire occurred image datasets is still challenge. Training deep learning model in this environment, can lead mis-detection when burning point is far from the camera or according to objects similar to flame and smoke. Our study attempted to create synthetic wildfire images in various shapes by inserting damage into a free-wildfire image using generative adversarial network (GAN) and Weakly supervised object localization (WSOL). The synthesized image can used as training data for object detection by applying the WSOL method with gradient-weighted activation map (Grad-CAM). Additionally, the YOLOv5s model was improved by adding a channel attention module; sequence-and-excitation (SE) layer and replace loss function as CIoU to address the issue of wildfire false detection in fire-like object and miss detection in small size smoke. Our proposed method, produced results as high as 7.19% in F1-score and 6.41% in average precision (AP) when compared to the existing traditional method. To use a deep learning model in practice, a lightweight model should be applied to the embedded models while maintaining high performance. The developed AI model was applied to the established drone and CCTV-based wildfire monitoring system, and a virtual experiment was conducted by generating virtual wildfires near forests in Korea.}
}
@article{RECKLING2023100936,
title = {Predicting residential septic system malfunctions for targeted drone inspections},
journal = {Remote Sensing Applications: Society and Environment},
volume = {30},
pages = {100936},
year = {2023},
issn = {2352-9385},
doi = {https://doi.org/10.1016/j.rsase.2023.100936},
url = {https://www.sciencedirect.com/science/article/pii/S2352938523000186},
author = {William Reckling and Jay Levine and Stacy A.C. Nelson and Helena Mitasova},
keywords = {Unmanned aerial system, UAS, Drone, Predictive modeling, Machine learning, Septic system malfunction},
abstract = {Septic system malfunctions can cause untreated sewage to pond in yards or contaminate drinking water wells leading to environmental and health problems. While most malfunction detections rely on reports by individuals, machine learning and remote sensing can be used to identify potentially failing systems. We propose a methodology that combines a machine learning technique implemented in Maxent with unmanned aerial system (UAS) mapping to create a priority queue for inspection and detecting malfunctions apparent in the collected imagery. We demonstrate the approach in Wake County, North Carolina, a County with 73,347 septic systems located within drinking water supply watersheds. The predictive modeling identified 102 systems with a 99.9% probability of failure. Four properties from the queue were mapped by UAS and the acquired imagery was visually analyzed in the visible spectrum for signs of malfunction. Our results suggest that the proposed approach can assist in the early identification of failing systems minimizing the environmental impacts and saving resource time and funds.}
}
@article{YAACOUB2020100218,
title = {Security analysis of drones systems: Attacks, limitations, and recommendations},
journal = {Internet of Things},
volume = {11},
pages = {100218},
year = {2020},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2020.100218},
url = {https://www.sciencedirect.com/science/article/pii/S2542660519302112},
author = {Jean-Paul Yaacoub and Hassan Noura and Ola Salman and Ali Chehab},
keywords = {UAV, UAS, UUV, Armed drones, Drone/UAV warfare, Terrorism/counter-Terrorism, Drones security, Drones threats and vulnerabilities, Drones attacks, Drones security countermeasures techniques, Drones forensics},
abstract = {Recently, the world witnessed a significant increase in the number of used drones, with a global and continuous rise in the demand for their multi-purpose applications. The pervasive aspect of these drones is due to their ability to answer people’s needs. Drones are providing users with a bird’s eye that can be activated and used almost anywhere and at any time. However, recently, the malicious use of drones began to emerge among criminals and cyber-criminals alike. The probability and frequency of these attacks are both high and their impact can be very dangerous with devastating effects. Therefore, the need for detective, protective and preventive counter-measures is highly required. The aim of this survey is to investigate the emerging threats of using drones in cyber-attacks, along the countermeasures to thwart these attacks. The different uses of drones for malicious purposes are also reviewed, along the possible detection methods. As such, this paper analyzes the exploitation of drones vulnerabilities within communication links, as well as smart devices and hardware, including smart-phones and tablets. Moreover, this paper presents a detailed review on the drone/Unmanned Aerial Vehicle (UAV) usage in multiple domains (i.e civilian, military, terrorism, etc.) and for different purposes. A realistic attack scenario is also presented, which details how the authors performed a simulated attack on a given drone following the hacking cycle. This review would greatly help ethical hackers to understand the existing vulnerabilities of UAVs in both military and civilian domains. Moreover, it allows them to adopt and come up with new techniques and technologies for enhanced UAV attack detection and protection. As a result, various civilian and military anti-drones/UAVs (detective and preventive) countermeasures will be reviewed.}
}
@article{ZHAO2024100691,
title = {Artificial intelligence for geoscience: Progress, challenges, and perspectives},
journal = {The Innovation},
volume = {5},
number = {5},
pages = {100691},
year = {2024},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2024.100691},
url = {https://www.sciencedirect.com/science/article/pii/S2666675824001292},
author = {Tianjie Zhao and Sheng Wang and Chaojun Ouyang and Min Chen and Chenying Liu and Jin Zhang and Long Yu and Fei Wang and Yong Xie and Jun Li and Fang Wang and Sabine Grunwald and Bryan M. Wong and Fan Zhang and Zhen Qian and Yongjun Xu and Chengqing Yu and Wei Han and Tao Sun and Zezhi Shao and Tangwen Qian and Zhao Chen and Jiangyuan Zeng and Huai Zhang and Husi Letu and Bing Zhang and Li Wang and Lei Luo and Chong Shi and Hongjun Su and Hongsheng Zhang and Shuai Yin and Ni Huang and Wei Zhao and Nan Li and Chaolei Zheng and Yang Zhou and Changping Huang and Defeng Feng and Qingsong Xu and Yan Wu and Danfeng Hong and Zhenyu Wang and Yinyi Lin and Tangtang Zhang and Prashant Kumar and Antonio Plaza and Jocelyn Chanussot and Jiabao Zhang and Jiancheng Shi and Lizhe Wang},
keywords = {artificial intelligence, machine learning, deep learning, geoscience},
abstract = {This paper explores the evolution of geoscientific inquiry, tracing the progression from traditional physics-based models to modern data-driven approaches facilitated by significant advancements in artificial intelligence (AI) and data collection techniques. Traditional models, which are grounded in physical and numerical frameworks, provide robust explanations by explicitly reconstructing underlying physical processes. However, their limitations in comprehensively capturing Earth’s complexities and uncertainties pose challenges in optimization and real-world applicability. In contrast, contemporary data-driven models, particularly those utilizing machine learning (ML) and deep learning (DL), leverage extensive geoscience data to glean insights without requiring exhaustive theoretical knowledge. ML techniques have shown promise in addressing Earth science-related questions. Nevertheless, challenges such as data scarcity, computational demands, data privacy concerns, and the “black-box” nature of AI models hinder their seamless integration into geoscience. The integration of physics-based and data-driven methodologies into hybrid models presents an alternative paradigm. These models, which incorporate domain knowledge to guide AI methodologies, demonstrate enhanced efficiency and performance with reduced training data requirements. This review provides a comprehensive overview of geoscientific research paradigms, emphasizing untapped opportunities at the intersection of advanced AI techniques and geoscience. It examines major methodologies, showcases advances in large-scale models, and discusses the challenges and prospects that will shape the future landscape of AI in geoscience. The paper outlines a dynamic field ripe with possibilities, poised to unlock new understandings of Earth’s complexities and further advance geoscience exploration.}
}
@article{AENDIKOV2024691,
title = {Integration of GIS and machine learning analytics into Streamlit application},
journal = {Procedia Computer Science},
volume = {231},
pages = {691-696},
year = {2024},
note = {14th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 13th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (EUSPN/ICTH 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.12.160},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923021725},
author = {Noyan Aendikov and Aeila Azayeva},
keywords = {Geographic Information Systems, Terrain Classification, Machine Learning, Deep Learning, Google Cloud Computing, Streamlit},
abstract = {This paper introduces an implementation of different GIS tools into Streamlit application: FCNN Terrain Classification, Earth Engine Dataset Parsing, and GIS Timelapse Animations. This toolkit is integrated with terrain multi-classification models using Fully Convolutional Neural Networks (FCNNs) for imagery data into Streamlit microservices. The proposed methodology involves labeled and unlabeled data collection from ESA WorldCover and Sentinel-2 MSI on the Google Earth Engine, compressing datasets into TFRecords format with 9 diverse terrain types, and handling Google Cloud training computations. The experimental results demonstrate the effectiveness of the CNN-based approach, achieving a tolerable from 60% up to 80% accuracy of the model and robust classification performance. The simplicity and efficiency of the proposed method make it suitable for real-world tasks requiring reliable and fast GIS analytics.}
}
@article{REHMAN20232289,
title = {Convolutional Neural Network Model for Fire Detection in Real-Time Environment},
journal = {Computers, Materials and Continua},
volume = {77},
number = {2},
pages = {2289-2307},
year = {2023},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.036435},
url = {https://www.sciencedirect.com/science/article/pii/S1546221823006446},
author = {Abdul Rehman and Dongsun Kim and Anand Paul},
keywords = {Fire detection, industrial surveillance system, smart devices, smart social agent (SSA), machine learning algorithms, CNN},
abstract = {Disasters such as conflagration, toxic smoke, harmful gas or chemical leakage, and many other catastrophes in the industrial environment caused by hazardous distance from the peril are frequent. The calamities are causing massive fiscal and human life casualties. However, Wireless Sensors Network-based adroit monitoring and early warning of these dangerous incidents will hamper fiscal and social fiasco. The authors have proposed an early fire detection system uses machine and/or deep learning algorithms. The article presents an Intelligent Industrial Monitoring System (IIMS) and introduces an Industrial Smart Social Agent (ISSA) in the Industrial SIoT (ISIoT) paradigm. The proffered ISSA empowers smart surveillance objects to communicate autonomously with other devices. Every Industrial IoT (IIoT) entity gets authorization from the ISSA to interact and work together to improve surveillance in any industrial context. The ISSA uses machine and deep learning algorithms for fire-related incident detection in the industrial environment. The authors have modeled a Convolutional Neural Network (CNN) and compared it with the four existing models named, FireNet, Deep FireNet, Deep FireNet V2, and Efficient Net for identifying the fire. To train our model, we used fire images and smoke sensor datasets. The image dataset contains fire, smoke, and no fire images. For evaluation, the proposed and existing models have been tested on the same. According to the comparative analysis, our CNN model outperforms other state-of-the-art models significantly.}
}
@article{RAMSEY2024101274,
title = {Mapping the recovery of Mountain Ash (Eucalyptus regnans) and Alpine Ash (E. delegatensis) using satellite remote sensing and a machine learning classifier},
journal = {Remote Sensing Applications: Society and Environment},
volume = {36},
pages = {101274},
year = {2024},
issn = {2352-9385},
doi = {https://doi.org/10.1016/j.rsase.2024.101274},
url = {https://www.sciencedirect.com/science/article/pii/S2352938524001381},
author = {Simon Ramsey and Karin Reinke and Simon Jones},
keywords = {Earth observation, Eucalyptus, Random forests, Disturbance, Logging},
abstract = {This research presents a random forest classification approach to map the response of the obligate-seeder Eucalyptus species, Mountain Ash (Eucalyptus regnans) and Alpine Ash (E. delegatensis), to disturbance from timber harvesting in the Victorian Central Highlands in south-eastern Australia. A Sentinel-2 MultiSpectral Instrument (MSI) composite image was classified and analysed using a random forest algorithm trained using field data collected within fifty-three sites. Training and validation datasets were produced by randomly sub setting using a 70:30 split. Validation was performed by producing a confusion matrix using the points which were excluded from model training. The random forest model demonstrated strong performance at distinguishing Eucalyptus regrowth from the dominant understory species, Silver Wattle (Acacia dealbata), achieving an F1-score of 97.3% and true skill statistic of 96.4%. This study showcases the operational insights that satellite remote sensing data and machine learning can provide for regional-scale monitoring and management of E. regnans and E. delegatensis dominant ecosystems following disturbance. Due to the high conservation value of these communities, and their sensitivity to frequent high intensity disturbance and low precipitation during regeneration, this research seeks to provide a means to assess the condition of regenerating forest and in doing so enhance our understanding of these ecologically significant ecosystems in response to changing environmental conditions.}
}
@article{RATHOD2023e00479,
title = {Multipurpose deep learning-powered UAV for forest fire prevention and emergency response},
journal = {HardwareX},
volume = {16},
pages = {e00479},
year = {2023},
issn = {2468-0672},
doi = {https://doi.org/10.1016/j.ohx.2023.e00479},
url = {https://www.sciencedirect.com/science/article/pii/S246806722300086X},
author = {Tejas Rathod and Vinay Patil and R. Harikrishnan and Priti Shahane},
keywords = {UAV, Deep learning, GANs, Forest fire prediction},
abstract = {This paper presents a customized UAV designed for rescue and safety purposes in the forest sector. The UAV features a durable F450 frame quadcopter with four 1000KV brushless motors and a KK2.1 Flight Control Board for stability and manoeuvrability with a runtime of 90 min. It incorporates a Raspberry Pi camera for real-time video streaming, enabling efficient identification of individuals in need of assistance. The GSM module allows contactless communication, ensuring streamlined and safe interaction. A motor controls the lid of the customizable first aid kit box, facilitating efficient aid delivery. The Neo-6 M GPS module provides accurate localization of the drone and individuals in distress with a horizontal position accuracy of 2.5 m. The UAV collects temperature and humidity data using the DHT 11 sensor having +/- 2 degreesC and +- 5% accuracy respectively. This sensor employs advanced deep learning models, including artificial neural networks (ANN) and generative adversarial networks (GANs), for real-time forest fire prediction with an accuracy of 90.7 % The integration of GANs enhances accuracy through synthetic data generation. Moreover, all these components are interfaced using a Raspberry Pi4 and a GUI, providing a smooth user control experience and end-to-end information for quick and effective emergencyresponse.}
}
@article{JONNALAGADDA2024101181,
title = {SegNet: A segmented deep learning based Convolutional Neural Network approach for drones wildfire detection},
journal = {Remote Sensing Applications: Society and Environment},
volume = {34},
pages = {101181},
year = {2024},
issn = {2352-9385},
doi = {https://doi.org/10.1016/j.rsase.2024.101181},
url = {https://www.sciencedirect.com/science/article/pii/S2352938524000454},
author = {Aditya V. Jonnalagadda and Hashim A. Hashim},
keywords = {Segment Neural Network, Machine learning, Unmanned Aerial Vehicle, Convolution Neural Network, Wildfire, Detection, Computer vision},
abstract = {This research addresses the pressing challenge of enhancing processing times and detection capabilities in Unmanned Aerial Vehicle (UAV)/drone imagery for global wildfire detection, despite limited datasets. Proposing a Segmented Neural Network (SegNet) selection approach, we focus on reducing feature maps to boost both time resolution and accuracy significantly advancing processing speeds and accuracy in real-time wildfire detection. This paper contributes to increased processing speeds enabling real-time detection capabilities for wildfire, increased detection accuracy of wildfire, and improved detection capabilities of early wildfire, through proposing a new direction for image classification of amorphous objects like fire, water, smoke, etc. Employing Convolutional Neural Networks (CNNs) for image classification, emphasizing on the reduction of irrelevant features vital for deep learning processes, especially in live feed data for fire detection. Amidst the complexity of live feed data in fire detection, our study emphasizes on image feed, highlighting the urgency to enhance real-time processing. Our proposed algorithm combats feature overload through segmentation, addressing challenges arising from diverse features like objects, colors, and textures. Notably, a delicate balance of feature map size and dataset adequacy is pivotal. Several research papers use smaller image sizes, compromising feature richness which necessitating a new approach. We illuminate the critical role of pixel density in retaining essential details, especially for early wildfire detection. By carefully selecting number of filters during training, we underscore the significance of higher pixel density for proper feature selection. The proposed SegNet approach is rigorously evaluated using real-world dataset obtained by a drone flight and compared to state-of-the-art literature.}
}
@article{MOMENI2022102859,
title = {Coordinated routing system for fire detection by patrolling trucks with drones},
journal = {International Journal of Disaster Risk Reduction},
volume = {73},
pages = {102859},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.102859},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922000784},
author = {Maryam Momeni and Hamed Soleimani and Shahrooz Shahparvari and Behrouz Afshar-Nadjafi},
keywords = {Unmanned aerial vehicles, Augmented epsilon constraint, VRPD, Benders' decomposition algorithm, Patrol, Bushfire},
abstract = {Vehicle routing problem with drones (VRPD) is an extension of classical vehicle routing, which involves the usage of trucks and drones. Nowadays, bushfires are taken into account as a major global challenging subject. Indeed, forests are very vulnerable to fires, and the classical patrol systems cannot patrol all areas of the forest and pastures due to the difficult crossing of some routes. The literature survey demonstrated that the mathematical models are an efficient approach for planning forest and rangeland patrols. Drones and trucks are used in cooperation for forest monitoring in this study, with drones acting as an efficient and accurate means of monitoring routes that are impassable to ground vehicles. However, this novel approach deals with several substantial challenges, including limitation in the drone's battery and the routing problem in an integrated truck-drone system based on considering hard-to-reach areas and creating a trade-off between time and cost of the patrol process. These issues make significant differences between the proposed novel approach and the classical vehicle routing problem. Thus, it is essential to develop a comprehensive approach to resolve these crucial issues. The present study develops a multi-objective mathematical programming model as mixed-integer programming for simultaneous patrolling of trucks and drones. The augmented epsilon constraint (AEC) method has been considered to solve the two-objective mathematical programming model. Besides, Benders' decomposition (BD) algorithm has been employed to evaluate the achieved results in large-size problems. The performance of the BD algorithm has been confirmed in this study. The findings indicated that the proposed model efficiently protected the forest via monitoring all areas, including impassable areas where ground patrolling is impossible (e.g., the lack of roads due to dense vegetation). Also, it was proved that this system was a novel, cost-effective patrol, which prevents uncontrollable fires in the shortest possible time.}
}
@article{MUSTAFA2024200430,
title = {Natural disasters detection using explainable deep learning},
journal = {Intelligent Systems with Applications},
volume = {23},
pages = {200430},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200430},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324001042},
author = {Ahmad M. Mustafa and Rand Agha and Lujain Ghazalat and Tariq Sha'ban},
keywords = {Image classification, Natural disasters classification, Transfer learning, Explainable AI},
abstract = {Deep learning applications have far-reaching implications in people’s daily lives. Disaster management professionals are becoming increasingly interested in applying deep learning to prepare for and respond to natural disasters. In this paper, we aim to assist natural disaster management professionals in preparing for disasters by developing a framework that can accurately classify natural disasters and interpret the results using a combination of a deep learning model and an XAI method to ensure reliability and ease of interpretation without a technical background. Two main aspects categorize the novelty of our work. The first is utilizing pre-trained Models such as VGGNet19, ResNet50, and ViT for accurate classification of natural disaster images. The second is implementing three explainable AI techniques-Gradient-weighted Class Activation Mapping (Grad-CAM), Grad CAM++, and Local Interpretable Model-agnostic Explanations (LIME) to ensure the interpretability of the model’s predictions, making the decision-making process transparent and reliable. Experiments on the Natural disaster datasets (Niloy et al. 2021) and MEDIC with a ViT-B-32 model achieved a high accuracy of 95.23%. Additionally, explainable artificial intelligence techniques such as LIME, Grad-CAM, and Grad-CAM++ are used to evaluate model performance and visualize decision-making. Our code is available at.11https://github.com/tariqshaban/disaster-classification-with-xai.}
}
@article{RAMADAN2024101248,
title = {Towards early forest fire detection and prevention using AI-powered drones and the IoT},
journal = {Internet of Things},
volume = {27},
pages = {101248},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101248},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524001896},
author = {Montaser N.A. Ramadan and Tasnim Basmaji and Abdalla Gad and Hasan Hamdan and Bekir Tevfik Akgün and Mohammed A.H. Ali and Mohammad Alkhedher and Mohammed Ghazal},
keywords = {Internet of Things (IoT), LoRaWAN network, Wildfire, Artificial intelligence (AI), Fire detection, Unmanned aerial vehicle (UAV), Autonomous navigation},
abstract = {Wildfires are a significant natural hazard, resulting in financial losses, human deaths, and environmental damage. Due to the rising severity and frequency of wildfires, wildfire management and detection have recently received increased attention worldwide. Monitoring potential risk areas and early fire detection are critical factors for shortening the reaction time and reducing the potential damage. Conventional wildfire detection techniques like satellite imaging and remote camera-based sensing need more latency and low reliability. To tackle these limitations, this paper proposes a novel airborne UAV-based IoT (UIoT) system for wildfire sensing, detection, and extinguishing. It presents the design of low-cost and low-maintenance fire-detecting IoT nodes for large-scale deployment. It also proposes, investigates, and reports on several connectivity architectures using the LoRaWAN protocol for UIoT systems. We geolocate forest trees in a terrain-mapping exercise for precise fire localization. We then deploy an autonomous drone with a visual camera that utilizes our novel classification network to detect the presence of fire followed by fire detection and particle filter-based tracking to center fire at the center of the frame. We use a cloud back-end server for monitoring, analysis, and reporting. Our results show that our low-cost and long-range IoT nodes accurately detect fire within 1-5 min after fire ignition. Our fire classification network achieved an accuracy of 99.46% and a mean average precision of 99.64%. Numerical results suggest that the proposed UAV-IoT-based fire classification offers a fast and reliable wildfire recognition and extinguishing solution while minimizing power consumption and increasing the battery lifespan.}
}
@article{EDULAKANTI2023100464,
title = {Review article: The emerging drone technology and the advancement of the Indian drone business industry},
journal = {The Journal of High Technology Management Research},
volume = {34},
number = {2},
pages = {100464},
year = {2023},
issn = {1047-8310},
doi = {https://doi.org/10.1016/j.hitech.2023.100464},
url = {https://www.sciencedirect.com/science/article/pii/S1047831023000147},
author = {Srinivas Reddy Edulakanti and Sanjeev Ganguly},
keywords = {Emerging business, Drones, Unmanned Aerial Vehicle, Drone technology, Business strategy, Commercial drones, Military drones, Emerging technology, UAV, Global industry, Unmanned Aerial System, UAS},
abstract = {Undoubtedly, the drone industry is one of the fastest-growing industries in the world today. There is unlimited potential for Drone technology with continued growth and investment which are essential to categorize drones as an emerging technology. So, the drone industry is the strongest case for an emerging business industry. The number of industries benefiting from drone technology continues to grow. The emerging drone technology and the advancement of the Indian drone business industry are cause and effect relation which are growing and making positive impact across the global drone business industry. We provide an overview and interrelationship of the emerging drone technology and advancement of Indian drone business industry as there is no review to date, has offered a wholistic retrospection of this kind of research review and address this gap. So, this manuscript aims to provide readers with a high-level overview and review of business developments in widely available unmanned aerial vehicles (UAVs), as well as a short summary of the global drone industry and studies that have been covered on drone business industry growth in India over the past decade. This review paper provides a guide that can be used to make sense of the emerging drone business industry and its effect on ever growing drone business in India. The purpose of this review report is to provide a comprehensive market study for the drone business industry that covers a variety of topics, such as relevant facts, relevant historical data, industry-validated market statistics, and predictions based on a systematic literature review (SLR) methodology and set of assumptions that are acceptable. This literature review is longitudinal, and qualitative in nature.}
}
@article{SHAMSOSHOARA2021108001,
title = {Aerial imagery pile burn detection using deep learning: The FLAME dataset},
journal = {Computer Networks},
volume = {193},
pages = {108001},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108001},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621001201},
author = {Alireza Shamsoshoara and Fatemeh Afghah and Abolfazl Razi and Liming Zheng and Peter Z. Fulé and Erik Blasch},
keywords = {Aerial imaging, Fire monitoring dataset, Fire detection and segmentation, Deep learning},
abstract = {Wildfires are one of the costliest and deadliest natural disasters in the US, causing damage to millions of hectares of forest resources and threatening the lives of people and animals. Of particular importance are risks to firefighters and operational forces, which highlights the need for leveraging technology to minimize danger to people and property. FLAME (Fire Luminosity Airborne-based Machine learning Evaluation) offers a dataset of aerial images of fires along with methods for fire detection and segmentation which can help firefighters and researchers to develop optimal fire management strategies. This paper provides a fire image dataset collected by drones during a prescribed burning piled detritus in an Arizona pine forest. The dataset includes video recordings and thermal heatmaps captured by infrared cameras. The captured videos and images are annotated, and labeled frame-wise to help researchers easily apply their fire detection and modeling algorithms. The paper also highlights solutions to two machine learning problems: (1) Binary classification of video frames based on the presence [and absence] of fire flames. An Artificial Neural Network (ANN) method is developed that achieved a 76% classification accuracy. (2) Fire detection using segmentation methods to precisely determine fire borders. A deep learning method is designed based on the U-Net up-sampling and down-sampling approach to extract a fire mask from the video frames. Our FLAME method approached a precision of 92%, and recall of 84%. Future research will expand the technique for free burning broadcast fire using thermal images.}
}
@incollection{FEEHAN2025239,
title = {Chapter 12 - Future trends in computational data analytics and artificial intelligence for Earth resource management},
editor = {Deepak Kumar and Tavishi Tewary and Sulochana Shekhar},
booktitle = {Data Analytics and Artificial Intelligence for Earth Resource Management},
publisher = {Elsevier},
pages = {239-272},
year = {2025},
isbn = {978-0-443-23595-5},
doi = {https://doi.org/10.1016/B978-0-443-23595-5.00012-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443235955000127},
author = {Madison C. Feehan and Deepak Kumar},
keywords = {Computational analytics, artificial intelligence, climate technology, machine learning, earth science, resource management},
abstract = {The future trends in computational data analytics and Artificial Intelligence (AI) for Earth resource management promise groundbreaking advancements. As technology evolves, data analytics will focus on integrating diverse and expansive datasets, employing advanced algorithms for more comprehensive analysis. AI will further revolutionize predictive modeling by leveraging deep learning techniques, offering more precise forecasts of environmental changes. The fusion of AI with remote sensing technologies will enable real-time, high-resolution monitoring of the Earth’s surface, facilitating immediate responses to environmental threats. Ethical AI development will be paramount, ensuring transparency and fairness in decision-making processes. Collaborative research endeavors will drive innovation, promoting sustainability and resilience in Earth resource management. These future trends herald a transformative era, empowering stakeholders with more accurate insights and adaptive strategies, ultimately fostering a harmonious coexistence between humanity and the environment.}
}
@article{JIAO2023109540,
title = {Audio features based ADS-CNN method for flight attitude recognition of quadrotor UAV},
journal = {Applied Acoustics},
volume = {211},
pages = {109540},
year = {2023},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2023.109540},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X23003389},
author = {Qingchun Jiao and Xiaolong Wang and Lijun Wang and Huihui Bai},
keywords = {Deep learning, Depthwise separable convolution, Attention mechanism, UAV flight attitude detection, Voice recognition},
abstract = {With the rapid development of unmanned flight technology, it has been widely used in all walks of life. Quad-rotor unmanned aerial vehicle (QrUAV) is the most widely used, due to its advantages of simple structure, high stability, and easy control. Currently, efficient, fast and accurate identification of QrUAV is still a hot issue. To this end, this paper proposes an audio features based flight attitude recognition method for QrUAV. Using the newly recorded UAV Attitude Audio data set (UAVAA), a lightweight convolutional neural network (ADS-CNN) integrated with attention mechanism is established for the flight attitude recognition of QrUAV. The feature of this method is that a lightweight network structure is designed through depthwise separable convolution and residual connection, this structure effectively reduces the network parameter consumption, and increases the network depth as much as possible when the number of UAVAA samples is small, which effectively suppresses overfitting and improves the recognition accuracy. In addition, by introducing of the region of interest focusing module (IFA) into the network, the weight of the attitude features is divided to realize the importance of the features, so as to achieve efficient and high-accuracy recognition. Finally, by fusing the MFCC features and STFT features of the QrUAV audio, the feature dimension is increased and the recognition accuracy is improved. By experimental comparison and analysis, the accuracy rate of ADS-CNN reached 98.81%, which was 2.7% higher than that of VGG16, the model parameters of ADS-CNN were only 27.6% of VGG16, and the operation time was only 32% of VGG16, which shows that ADS –CNN can identify the flight attitude of the QrUAV more accurately and quickly by using attitude audio.}
}
@article{CHINTHIREDDY2022100459,
title = {DarkSky: Privacy-preserving target tracking strategies using a flying drone},
journal = {Vehicular Communications},
volume = {35},
pages = {100459},
year = {2022},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2022.100459},
url = {https://www.sciencedirect.com/science/article/pii/S2214209622000067},
author = {Samhith Reddy Chinthi-Reddy and Sunho Lim and Gyu Sang Choi and Jinseok Chae and Cong Pu},
keywords = {Drone, Location privacy, Target tracking, Unmanned aerial vehicle (UAV)},
abstract = {Commercially well-known drones, unmanned aerial vehicles (UAVs), are increasingly popular with the public and have been widely deployed in diverse applications. However, a drone equipped with tracking, monitoring, or sensing device(s) can illegally collect privacy- and security-sensitive information and intrude restricted areas. Thus, recent literature focuses on the protection of users and restricted areas from an unwanted privacy attack and intrusion caused by the drone. Unlike prior research, however, we fundamentally shift the privacy paradigm from protecting users and restricted areas from a malicious drone into protecting and hiding the sensitive information of a drone from an adversary. In light of these, we propose three privacy-preserving target tracking strategies based on the shortest path, random locations, and dummy locations. The basic idea is to obfuscate the current location of the drone and randomize the trajectory to prevent the adversary from locating and tracking the drone. We also analyze drone privacy in terms of location and trajectory and measure them through entropy-based anonymity, the size of convex-hull, and the number of paths. We conduct extensive simulation experiments using the OMNeT++ for performance evaluation and comparison with stationary and moving target tracking scenarios under three mobility models. The simulation results indicate that the proposed strategies can be a viable approach to track the target while reserving a certain level of location and trajectory privacy.}
}
@article{MOMENI2024101168,
title = {Collaboration of thermal sensors and drones in fighting wildfires; Mathematical model and heuristic approach},
journal = {Internet of Things},
volume = {26},
pages = {101168},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101168},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524001094},
author = {Maryam Momeni and S. Mohammad J. {Mirzapour Al-e-Hashem}},
keywords = {Wildfires, Deforestation, Forest fire response, Collaboration of thermal sensors and drones, heuristic},
abstract = {The forest temperature is monitored utilizing thermal sensors. In the event of a temperature increase, these sensors promptly notify firefighters, enabling timely intervention against fires. Drones offer a potential solution for fire containment by swiftly addressing them in their nascent stages, as identified by temperature fluctuations detected by these sensors. Alternatively, if left unchecked, the situation may necessitate protracted aerial operations like the deployment of fixed-wing air tankers, helicopters, smokejumpers, and rappellers. Regrettably, such measures consume invaluable time that could be better spent combating the blaze. Recent literature on wildfires underscores the significance of collaborative approaches and the seamless integration of contemporary technologies in mitigating the expenses associated with forest fires. This study builds upon the temperature data acquired through thermal sensors to propose an all-encompassing mathematical framework designed to identify and facilitate the early extinguishment of fires using drones. The initial step involves formulating the problem through a mixed-integer linear programming model. To solve the model within expansive dimensions, a heuristic approach inspired by Clark and Wright is posited. The efficacy of this algorithm is gauged through the resolution of diverse scenarios. Promisingly, the results show the viability of collaboration of thermal sensors and drones to accurately pinpoint and extinguish forest fires. The outcomes underscore not only the protective capabilities of the suggested methodology but also its cost-effectiveness. Moreover, the approach's pioneering nature is underscored as it offers a fiscally prudent strategy that rapidly curbs uncontrollable fires.}
}
@article{KUCHARCZYK2021112577,
title = {Remote sensing of natural hazard-related disasters with small drones: Global trends, biases, and research opportunities},
journal = {Remote Sensing of Environment},
volume = {264},
pages = {112577},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112577},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721002972},
author = {Maja Kucharczyk and Chris H. Hugenholtz},
keywords = {Drone, UAV, UAS, RPAS, Disaster, Hazard, Emergency, Risk, PRISMA},
abstract = {Small (< 25 kg) aerial drones have expanded the remote sensing toolkit for disaster management activities. Here, we provide a critical review of drone-based remote sensing of natural hazard-related disasters to highlight research trends, biases, and expose new opportunities. We performed a systematic literature search using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses methodology, resulting in 635 relevant articles from which we derived statistics relating to geography, drone hardware, disaster management application, and drone remote sensing data type and analysis method. Key findings include a bias towards: (i) mass movement hazards (38%); (ii) small (< 1 km2) (76%) and rural (79%) study areas in high-income countries and territories (64%); (iii) image-based observations of features from the natural environment (77%); and (iv) support of mitigation-related vulnerability assessment and risk modeling (54%) and environmental recovery (23%). We recommend that future studies focus on: (i) earthquakes, floods, and cyclones and other windstorms due to higher loss of life and economic impacts; (ii) larger and urban study areas in low, lower-middle, and upper-middle income countries and territories to support vulnerable populations; (iii) under-demonstrated (and especially response-related) disaster management activities, which generally require observations of built features from urban environments; and (iv) data standards for integrating drone-based remote sensing with international disaster management methodologies.}
}
@article{GHELICHI2022103735,
title = {Drone logistics for uncertain demand of disaster-impacted populations},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {141},
pages = {103735},
year = {2022},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.103735},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X2200170X},
author = {Zabih Ghelichi and Monica Gentili and Pitu B. Mirchandani},
keywords = {Drone delivery, Stochastic optimization, Chance constrained optimization, Humanitarian logistics},
abstract = {The paper introduces a stochastic optimization-based approach to address the logistics for the timely delivery of aid packages to disaster-affected areas utilizing a fleet of drones when the set of demand points is unknown. The major problem addressed is to locate a set of drone take-off platforms so that with a specified probability α, the maximum total disutility (or cost) under all realizations of the demand locations is minimized. A set of discrete scenarios defines the uncertainty set of the demand points. A Chance Constrained Programming (CCP) formulation is developed to select a set of platform locations whose disutility distribution produces minimum α percentile. For each platform location set, and each demand scenario, the total disutility is defined as the total delivery time for serving the demand points plus a penalty for unvisited demand points. For every set of drone platform locations, referred to as a candidate combination of platforms, the resultant disutility distribution is obtained by solving a space-time drone scheduling subproblem for all possible demand scenarios. The drone scheduling subproblem optimally schedules and sequences a set of trips for each drone so that the total disutility is minimized. Owing to the computational complexity of the proposed approach, an approximation method is developed that decomposes the problem into three tractable stages. The first stage identifies a set of most preferable platform combinations. The second stage develops an approximation algorithm based on a greedy approach to mitigate the extensive computational requirements for solving the large number of drone scheduling subproblems. The last stage builds upon the properties of a Sample Average Approximation (SAA) method and of the CCP formulation to select the optimum set of platforms. Finally, the performance of the proposed stochastic approach is evaluated through a series of computational experiments and a case study of Central Florida. The results reveal interesting insights and demonstrate the effectiveness of the proposed logistics system for drone delivery of humanitarian aid packages.}
}
@article{SABET2023104464,
title = {Scalable modular synthetic data generation for advancing aerial autonomy},
journal = {Robotics and Autonomous Systems},
volume = {166},
pages = {104464},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104464},
url = {https://www.sciencedirect.com/science/article/pii/S0921889023001033},
author = {Mehrnaz Sabet and Praveen Palanisamy and Sakshi Mishra},
keywords = {Aerial autonomy, Drone, Synthetic data, Sim-to-real, Domain randomization},
abstract = {One major barrier to advancing aerial autonomy has been collecting large-scale aerial datasets for training machine learning models. Due to costly and time-consuming real-world data collection through deploying drones, there has been an increasing shift towards using synthetic data for training models in drone applications. However, to increase widespread generalization and transferring models to real-world, increasing the diversity of simulation environments to train a model over all the varieties and augmenting the training data, has been proved to be essential. Current synthetic aerial data generation tools either lack data augmentation or rely heavily on manual workload or real samples for configuring and generating diverse realistic simulation scenes for data collection. These dependencies limit scalability of the data generation workflow. Accordingly, there is a major challenge in balancing generalizability and scalability in synthetic data generation. To address these gaps, we introduce a scalable Aerial Synthetic Data Augmentation (ASDA) framework tailored to aerial autonomy applications. ASDA extends a central data collection engine with two scriptable pipelines that automatically perform scene and data augmentations to generate diverse aerial datasets for different training tasks. ASDA improves data generation workflow efficiency by providing a unified prompt-based interface over integrated pipelines for flexible control. The procedural generative approach of our data augmentation is performant and adaptable to different simulation environments, training tasks and data collection needs. We demonstrate the effectiveness of our method in automatically generating diverse datasets and show its potential for downstream performance optimization. Our work contributes to generating enhanced benchmark datasets for training models that can generalize better to real-world situations. Video: youtube.com/watch?v=eKpOh-K-NfQ}
}
@article{AKHYAR2024112067,
title = {Deep artificial intelligence applications for natural disaster management systems: A methodological review},
journal = {Ecological Indicators},
volume = {163},
pages = {112067},
year = {2024},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2024.112067},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X24005247},
author = {Akhyar Akhyar and Mohd {Asyraf Zulkifley} and Jaesung Lee and Taekyung Song and Jaeho Han and Chanhee Cho and Seunghyun Hyun and Youngdoo Son and Byung-Woo Hong},
keywords = {Artificial intelligence, Deep learning, Neural network, Convolutional neural network, Semantic segmentation, Forest fire, Flood, Earthquake},
abstract = {Deep learning techniques through semantic segmentation networks have been widely used for natural disaster analysis and response. The underlying base of these implementations relies on convolutional neural networks (CNNs) that can accurately and precisely identify and locate the respective areas of interest within satellite imagery or other forms of remote sensing data, thereby assisting in disaster evaluation, rescue planning, and restoration endeavours. Most CNN-based deep-learning models encounter challenges related to the loss of spatial information and insufficient feature representation. This issue can be attributed to their suboptimal design of the layers that capture multiscale-context information and their failure to include optimal semantic information during the pooling procedures. In the early layers of CNNs, the network encodes elementary semantic representations, such as edges and corners, whereas, as the network progresses toward the later layers, it encodes more intricate semantic characteristics, such as complicated geometric shapes. In theory, it is advantageous for a segmentation network to extract features from several levels of semantic representation. This is because segmentation networks generally yield improved results when both simple and intricate feature maps are employed together. This study comprehensively reviews current developments in deep learning methodologies employed to segment remote sensing images associated with natural disasters. Several popular deep learning models, such as SegNet U-Net, FCNs, FCDenseNet, PSPNet, HRNet, and DeepLab, have exhibited notable achievements in various applications, including forest fire delineation, flood mapping, and earthquake damage assessment. These models demonstrate a high level of efficacy in distinguishing between different land cover types, detecting infrastructure that has been compromised or damaged, and identifying regions that are fire-susceptible to further dangers.}
}
@article{YAR2024104989,
title = {An efficient deep learning architecture for effective fire detection in smart surveillance},
journal = {Image and Vision Computing},
volume = {145},
pages = {104989},
year = {2024},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2024.104989},
url = {https://www.sciencedirect.com/science/article/pii/S0262885624000933},
author = {Hikmat Yar and Zulfiqar Ahmad Khan and Imad Rida and Waseem Ullah and Min Je Kim and Sung Wook Baik},
keywords = {Aerial view, Attention mechanism, Convolutional neural network, Environment monitoring, Fire detection, Remote sensing},
abstract = {The threat of fire is pervasive, poses significant risks to the environment, and may include potential fatalities, property devastation, and socioeconomic disruption. Successfully mitigating these risks relies on the prompt identification of fires, a process in which soft computing methodologies play a pivotal role. Although, these fire detection methodologies neglected to explore the relationships among fire-indicative features, which are important to enable a model to learn more representative and robust features in remote sensing scenarios. In the context of small fire detection from aerial view using satellite imagery or unmanned arial vehicle (UAVs) presents challenges to capture rich spatial detail, hinder the model ability for accurate fire scene classification. Furthermore, it is significant to manage model complexity effectively to facilitate deployment on UAVs for fast and accurate responses in an emergency situation. To cope with these challenges, we propose an advanced model integrated a modified soft attention mechanism (MSAM) and a 3D convolution operation with a MobileNet architecture to overcome obstacles related to optimising features and controlling model complexity. The MSAM enabling the model to selectively emphasise essential features during the training process which acts as a selective filter. This adaptive attention mechanism enhances sensitivity and allowing the model to prioritise relevant patterns for accurate fire detection. Concurrently, the integration of a 3D convolutional operation extends the model spatial awareness, to capture intricate details across multiple scales, and particularly in small regions observed from aerial viewpoints. Benchmark evaluations of the proposed model over the FD, DFAN, and ADSF datasets reveal superior performance with enhanced accuracy (ACR) compared to existing methods. Our model surpassed the state-of-the-art models with an average ACR improvement of 0.54%, 2.64%, and 1.20% on the FD, ADSF, and DFAN datasets, respectively. Furthermore, the use of an explainable artificial intelligence (XAI) technique enhances the validation of the model visual emphasis on critical regions of the image, providing valuable insights into its decision-making process.}
}
@article{ZENNARO2021103752,
title = {Exploring machine learning potential for climate change risk assessment},
journal = {Earth-Science Reviews},
volume = {220},
pages = {103752},
year = {2021},
issn = {0012-8252},
doi = {https://doi.org/10.1016/j.earscirev.2021.103752},
url = {https://www.sciencedirect.com/science/article/pii/S0012825221002531},
author = {Federica Zennaro and Elisa Furlan and Christian Simeoni and Silvia Torresan and Sinem Aslan and Andrea Critto and Antonio Marcomini},
keywords = {Machine learning, Climate change risk assessment, Big data, Remote sensing, Scientometric analysis, Systematic review},
abstract = {Global warming is exacerbating weather, and climate extremes events and is projected to aggravate multi-sectorial risks. A multiplicity of climate hazards will be involved, triggering cumulative and interactive impacts on a variety of natural and human systems. An improved understanding of risk interactions and dynamics is required to support decision makers in their ability to better manage current and future climate change risks. To face this issue, the research community has been starting to test new methodological approaches and tools, including the application of Machine Learning (ML) leveraging the potential of the large availability and variety of spatio-temporal big data for environmental applications. Given the increasing attention on the application of ML methods to Climate Change Risk Assessment (CCRA), this review mapped out the state of art and potential of these methods to this field of research. Scientometric and systematic analysis were jointly applied providing an in-depth review of publications across the 2000–2020 timeframe. The resulting output from the analysis showed that a huge variety of ML algorithms have been already applied within CCRA, among them, the most recurrent are Decision Tree, Random Forest, and Artificial Neural Network. These algorithms are often applied in an ensemble or hybridized way to analyze most of all floods and landslides risk events. Moreover, the application of ML to deal with remote sensing data is consistent and effective across reviewed CCRA applications, allowing the identification and classification of targets and the detection of environmental and structural features. On the contrary concerning future climate change scenarios, literature seems not to be very widespread into scientific production, compared to studies evaluating risks under current conditions. The same lack can be noted also for the assessment of cascading and compound hazards and risks, since these concepts are recently emerging in CCRA literature but not yet in combination with ML-based applications.}
}
@article{PEREIRA2024108928,
title = {Metaheuristic algorithms for calibration of two-dimensional wildfire spread prediction model},
journal = {Engineering Applications of Artificial Intelligence},
volume = {136},
pages = {108928},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108928},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624010868},
author = {Jorge Pereira and Jérôme Mendes and Jorge S.S. Júnior and Carlos Viegas and João Ruivo Paulo},
keywords = {Metaheuristic algorithms, Wildfire spread prediction, Model calibration, Remote sensing},
abstract = {Wildfires are complex phenomena with harmful consequences, ranging from environmental and property destruction to loss of human lives. In this sense, predicting wildfire behaviour is essential to mitigate its impacts and consequences. The Rothermel model is the most used fire rate of spread prediction model. However, input parameter uncertainty is a significant source of prediction error. In this paper, we propose the calibration of the input parameters of the fire propagation model by metaheuristic algorithms under a two-stage framework. The fire spread model consists on the Rothermel model in a two-dimensional approach for surface fires. The proposed calibration is performed in two stages iteratively repeated over time: (i) the calibration of the fire spread model’s input parameters and (ii) the wildfire spread prediction using the calibrated input parameters. The calibration was performed by the genetic algorithm, differential evolution, and simulated annealing, which calibrates the surface-area-to-volume ratio, fuel bed depth, live fuel moisture and dead fuel moisture. The symmetric difference between the real and predicted fire map shapes was defined as the fitness function of all three metaheuristic algorithms. For validation, simulations were done on two prescribed fires. The results for the real and estimated fire behaviour were then compared and revealed that all the tested metaheuristic algorithms produce a better fit to the real fire’s perimeter when compared to the uncalibrated Rothermel model. From the results, differential evolution provided the majority of best results when compared to genetic algorithm and simulated annealing algorithms in each scenario.}
}
@article{HORO2024102427,
title = {A Machine learning approach for Post-Disaster data curation},
journal = {Advanced Engineering Informatics},
volume = {60},
pages = {102427},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102427},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624000752},
author = {Sun {Ho Ro} and Yitong Li and Jie Gong},
keywords = {Natural disaster, Machine learning, Data curation, Image search},
abstract = {Image data collected after natural disasters play an important role in the forensics of structure failures. However, curating and managing large amounts of post-disaster imagery data is challenging. In most cases, data users still have to spend much effort to find and sort images from the massive amounts of images archived for past decades in order to study specific types of disasters. This paper proposes a new machine learning based approach for automating the labeling and classification of large volumes of post-natural disaster image data to address this issue. More specifically, the proposed method couples pre-trained computer vision models and a natural language processing model with an ontology tailed to natural disasters to facilitate the search and query of specific types of image data. The resulting process returns each image with five primary labels and similarity scores, representing its content based on the developed word-embedding model. Validation and accuracy assessment of the proposed methodology was conducted with ground-level residential building panoramic images from Hurricane Harvey. The computed primary labels showed a minimum average difference of 13.32% when compared to manually assigned labels. This versatile and adaptable solution offers a practical and valuable solution for automating image labeling and classification tasks, with the potential to be applied to various image classifications and used in different fields and industries. The flexibility of the method means that it can be updated and improved to meet the evolving needs of various domains, making it a valuable asset for future research and development.}
}
@article{YANG2024173273,
title = {Advancements in remote sensing for active fire detection: A review of datasets and methods},
journal = {Science of The Total Environment},
volume = {943},
pages = {173273},
year = {2024},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2024.173273},
url = {https://www.sciencedirect.com/science/article/pii/S004896972403420X},
author = {Songxi Yang and Qunying Huang and Manzhu Yu},
keywords = {Review, Remote sensing, Active fire detection, Machine learning, Deep learning},
abstract = {This study comprehensively and critically reviews active fire detection advancements in remote sensing from 1975 to the present, focusing on two main perspectives: datasets and corresponding instruments, and detection algorithms. The study highlights the increasing role of machine learning, particularly deep learning techniques, in active fire detection. Looking forward, the review outlines current challenges and future research opportunities in remote sensing for active fire detection. These include exploring data quality management and multi-modal learning, developing spatiotemporally explicit models, investigating self-supervised learning models, improving explainable and interpretable models, integrating physical-process based models with machine learning, and building digital twins to replicate wildfire dynamics and perform what-if scenario analysis. The review aims to serve as a valuable resource for informing natural resource management and enhancing environmental protection efforts through the application of remote sensing technology.}
}
@article{DELAFUENTE2024435,
title = {An optimization-based approach for an integrated forest fire monitoring system with multiple technologies and surveillance drones},
journal = {European Journal of Operational Research},
volume = {313},
number = {2},
pages = {435-451},
year = {2024},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2023.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0377221723006124},
author = {Rodrigo {De la Fuente} and Maichel M. Aguayo and Carlos Contreras-Bolton},
keywords = {Forest fire monitoring, Location-or-routing problem, Matheuristic, Drones, Surveillance towers},
abstract = {Wildfires pose a significant threat to forests and the delicate natural balance. In this work, we present a novel optimization approach for designing forest fire monitoring systems that incorporate surveillance towers, monitoring balloons, and drone technology, which have gained popularity in recent years for monitoring, logistics, and civil applications. We develop a compact mixed-integer linear programming model that includes location decisions for multiple monitoring technologies and routing of drones among monitoring facilities to achieve the most extensive possible terrain coverage. Additionally, we propose a matheuristic algorithm that comprises six components: a solution procedure, perturbation procedures, local search procedures, a call to the general-purpose solver to solve a mixed-integer linear programming model, a global reset strategy, a local reset strategy, and an acceptance criterion. We test the proposed model and algorithm on a set of random instances and a real-life case study in Chile. While the proposed model can only solve small instances, the matheuristic can find good-quality solutions for all instances. This study can aid the government and private sector in designing an integrated fire monitoring system that leverages watchtowers, monitoring balloons, and drones.}
}
@article{PURTELL2024102569,
title = {Bibliometric analysis on advanced air mobility and drones},
journal = {Journal of Air Transport Management},
volume = {116},
pages = {102569},
year = {2024},
issn = {0969-6997},
doi = {https://doi.org/10.1016/j.jairtraman.2024.102569},
url = {https://www.sciencedirect.com/science/article/pii/S0969699724000346},
author = {Clinton Purtell and Seock-Jin Hong and Brian Hiatt}
}
@article{JEMMALI20231562,
title = {Optimizing Forest Fire Prevention: Intelligent Scheduling Algorithms for Drone-Based Surveillance System},
journal = {Procedia Computer Science},
volume = {225},
pages = {1562-1571},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.145},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923013030},
author = {Mahdi Jemmali and B. Melhim {Loai Kayed} and Wadii Boulila and Hajer Amdouni and Mafawez T. Alharbi},
keywords = {Forest, fire, monitoring, drone, algorithms},
abstract = {Given the importance of forests and their role in maintaining the ecological balance, which directly affects the planet, the climate, and the life on this planet, this research presents the problem of forest fire monitoring using drones. The forest monitoring process is performed continuously to track any changes in the monitored region within the forest. During fires, drones’ capture data is used to increase the follow-up speed and enhance the control process of these fires to prevent their spread. The time factor in such problems determines the success rate of the fire extinguishing process, as appropriate data at the right time may be the decisive factor in controlling fires, preventing their spread, extinguishing them, and limiting their losses. Therefore, this research presented the problem of monitoring task scheduling for drones in the forest monitoring system. This problem is solved by developing several algorithms with the aim of minimizing the total completion time required to carry out all the drones’ assigned tasks. System performance is measured by using 990 instances of three different classes. The performed experimental results indicated the effectiveness of the proposed algorithms and their ability to act efficiently to achieve the desired goal. The algorithm RID achieved the best performance with a percentage rate of up to 90.3% with a time of 0.088 seconds.}
}
@article{MIRALLES20234959,
title = {A critical review on the state-of-the-art and future prospects of machine learning for Earth observation operations},
journal = {Advances in Space Research},
volume = {71},
number = {12},
pages = {4959-4986},
year = {2023},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2023.02.025},
url = {https://www.sciencedirect.com/science/article/pii/S027311772300145X},
author = {Pablo Miralles and Kathiravan Thangavel and Antonio {Fulvio Scannapieco} and Nitya Jagadam and Prerna Baranwal and Bhavin Faldu and Ruchita Abhang and Sahil Bhatia and Sebastien Bonnart and Ishita Bhatnagar and Beenish Batul and Pallavi Prasad and Héctor Ortega-González and Harrish Joseph and Harshal More and Sondes Morchedi and Aman {Kumar Panda} and Marco {Zaccaria Di Fraia} and Daniel Wischert and Daria Stepanova},
keywords = {Artificial Intelligence, Astrionics, Earth Observation, Edge Computing, Machine Learning, Neural Network, Remote Sensing, State-of-the-art},
abstract = {The continuing Machine Learning (ML) revolution indubitably has had a significant positive impact on the analysis of downlinked satellite data. Other aspects of the Earth Observation industry, despite being less susceptible to widespread application of Machine Learning, are also following this trend. These applications, actual use cases, possible prospects and difficulties, as well as anticipated research gaps, are the focus of this review of Machine Learning applied to Earth Observation Operations. A wide range of topics are covered, including mission planning, fault diagnosis, fault prognosis and fault repair, optimization of telecommunications, enhanced GNC, on-board image processing, and the use of Machine Learning models on platforms with constrained compute and power capabilities, as well as recommendations in the respective areas of research. The review tackles all on-board and off-board applications of machine learning to Earth Observation with one notable exception: it omits all post-processing of payload data on the ground, a topic that has been studied extensively by past authors. In addition, this review article discusses the standardization of Machine Learning (i.e., Guidelines and Roadmaps), as well as the challenges and recommendations in Earth Observation operations for the purpose of building better space missions.}
}
@article{TAVAKOLSADRABADI2024615,
title = {Enhancing wildfire propagation model predictions using aerial swarm-based real-time wind measurements: A conceptual framework},
journal = {Applied Mathematical Modelling},
volume = {130},
pages = {615-634},
year = {2024},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2024.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X24001227},
author = {Mohammad {Tavakol Sadrabadi} and Mauro Sebastián Innocente},
keywords = {Wildland fire, Fire spread, Fire-induced wind, Fire–wind coupling, Wind downscaling, Unmanned aerial vehicles},
abstract = {The dynamic behaviour of wildfires is mainly influenced by weather, fuel, and topography. Based on fundamental conservation laws involving numerous physical processes and large scales, atmospheric models require substantial computational resources. Therefore, coupling wildfire and atmospheric models is impractical for high resolutions. Instead, a static atmospheric wind field is typically input into the wildfire model, either as boundary conditions on the control surface or distributed over the control volume. Wildfire propagation models may be (i) data-driven; (ii) theoretical; or (iii) mechanistic surrogates. Data-driven models are beyond the scope of this paper. Theoretical models are based on conservation laws (species, energy, mass, momentum) and are, therefore, computationally intensive; e.g. the Fire Dynamics Simulator (FDS). Mechanistic surrogate models do not closely follow fire dynamics laws, but related laws observed to make predictions more efficiently with sufficient accuracy; e.g. FARSITE, and FDS with the Level Set model (FDS-LS). Whether theoretical or mechanistic surrogate, these wildfire models may be coupled with or decoupled from wind models (e.g. Navier-Stokes equations). Only coupled models account for the effect of the fire on the wind field. In this paper, a series of simulations of wildfire propagation on grassland are performed using FDS-LS to study the impact of the fire-induced wind on the fire propagation dynamics. Results show that coupling leads to higher Rates of Spread (RoS), closer to those reported from field experiments, with increasing wind speeds and higher terrain slopes strengthening this trend. Aiming to capture the fire–wind interaction without the hefty cost of solving Navier-Stokes equations, a conceptual framework is proposed: 1) a swarm of unmanned aerial vehicles measure wind velocities at flight height; 2) the wind field is constructed with the acquired data; 3) the high-altitude wind field is mapped to near-surface, and 4) the near-surface wind field is fed into the wildfire model periodically. A series of simulations are performed using an in-house decoupled physics-based reduced-order fire propagation model (FireProM-F) enhanced by wind field “measurements”. In this proof of concept, wind velocities are not measured but extracted from physics-based Large Eddy Simulations taken as ground truth. Unsurprisingly, higher measurement frequencies lead to more accurate and realistic predictions of the propagating fire front. An initial attempt is made to study the effect of wind measurement uncertainty on the model predictions by adding Gaussian noise. Preliminary results show that higher noise leads to the fire front displaying more irregular shapes and slower propagation.}
}
@article{MOHDDAUD202230,
title = {Applications of drone in disaster management: A scoping review},
journal = {Science & Justice},
volume = {62},
number = {1},
pages = {30-42},
year = {2022},
issn = {1355-0306},
doi = {https://doi.org/10.1016/j.scijus.2021.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1355030621001477},
author = {Sharifah Mastura Syed {Mohd Daud} and Mohd Yusmiaidil Putera {Mohd Yusof} and Chong Chin Heo and Lay See Khoo and Mansharan Kaur {Chainchel Singh} and Mohd Shah Mahmood and Hapizah Nawawi},
keywords = {Disaster, Drones, UAV, Humanitarian aid},
abstract = {The use of drones has rapidly evolved over the past decade involving a variety of fields ranging from agriculture, commercial and becoming increasingly used in disaster management or humanitarian aid. Unfortunately, the evidence of its use in mass disasters is still unclear and scarce. This article aims to evaluate the current drone feasibility projects and to discuss a number of challenges related to the deployment of drones in mass disasters in the hopes of empowering and inspiring possible future work. This research follows Arksey and O'Malley framework and updated by Joanna Briggs Institute Framework for Scoping Reviews methodology to summarise the results of 52 research papers over the past ten years, from 2009 to 2020, outlining the research trend of drone application in disaster. A literature search was performed in Medline, CINAHL, Scopus, individual journals, grey literature and google search with assessment based on their content and significance. Potential application of drones in disaster are broad. Based on articles identified, drone application in disasters are classified into four categories; (1) mapping or disaster management which has shown the highest contribution, (2) search and rescue, (3) transportation and (4) training. Although there is a significant increase in the number of publications on use of drone in disaster within the last five years, there is however limited discussion to address post-disaster healthcare situation especially with regards to disaster victim identification. It is evident that drone applications need to be further explored; to focus more on drone assistance to humans especially in victim identification. It is envisaged that with sufficient development, the application of drones appears to be promising and will improve their effectiveness especially in disaster management.}
}
@article{JIANG2024103750,
title = {Wildfire risk assessment using deep learning in Guangdong Province, China},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {128},
pages = {103750},
year = {2024},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2024.103750},
url = {https://www.sciencedirect.com/science/article/pii/S1569843224001043},
author = {Wenyu Jiang and Yuming Qiao and Xinxin Zheng and Jiahao Zhou and Juncai Jiang and Qingxiang Meng and Guofeng Su and Shaobo Zhong and Fei Wang},
keywords = {Wildfire, Deep learning, Risk assessment, Emergency management},
abstract = {The severe wildfires that have ravaged Guangdong province, China, present a significant threat to the local ecosystem, socio-economics, and public health. Effective risk assessment is essential for early warning and timely prevention in wildfire management, thereby mitigating disaster losses. In this study, we compiled a dataset comprising 11,507 historical wildfire incidents in Guangdong Province spanning a decade (2011–2021) and developed a deep learning-based model to predict the likelihood of wildfire occurrence in the region. In addition to analyzing risk characteristics throughout the year, we also trained separate models for different seasons and analyzed the discrepancies in the contribution of driven factors to wildfire occurrence across seasons. Furthermore, the performance of our deep learning-based model was compared with that of traditional machine learning algorithms. The experimental results revealed that: (1) Factors such as relative humidity, temperature, NDVI, and precipitation exerted significant influence on wildfire occurrence. (2) The impact of wildfire driving factors varied across different seasons. (3) Our deep learning model outperformed traditional machine learning models, achieving a superior performance with an AUC of 0.962.}
}
@incollection{KERLE2024,
title = {Remote Sensing for Disaster Risk Management—Advances and Limitations},
booktitle = {Reference Module in Earth Systems and Environmental Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-409548-9},
doi = {https://doi.org/10.1016/B978-0-443-13220-9.00044-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443132209000445},
author = {Norman Kerle and Marc {van den Homberg}},
keywords = {Damage assessment, Disaster risk management, Disaster response, Emergency response, Exposure, Hazard, Risk, Early action, Machine learning, Socio-natural disasters, UAV, Vulnerability},
abstract = {This chapter explains how the disaster domain has been a key application area for remote sensing since its pioneering days, with a strong development from opportunistic post-event image acquisition to concerted and regular monitoring of socio-natural hazards and comprehensive disaster risk management. It highlights recent technical and organizational advances, such as those related to dynamic, multi-hazard risk assessment, as well as large satellite constellations and sophisticated drones, with machine learning and semantic scene analysis playing increasing roles in data processing. Finally, the chapter discusses the limitations of remote sensing data and gaps in how they are used.}
}
@article{PI2020101009,
title = {Convolutional neural networks for object detection in aerial imagery for disaster response and recovery},
journal = {Advanced Engineering Informatics},
volume = {43},
pages = {101009},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.101009},
url = {https://www.sciencedirect.com/science/article/pii/S1474034619305828},
author = {Yalong Pi and Nipun D. Nath and Amir H. Behzadan},
keywords = {Disaster management, Convolutional neural network (CNN), Aerial reconnaissance, Deep learning, Unmanned aerial vehicle (UAV)},
abstract = {Accurate and timely access to data describing disaster impact and extent of damage is key to successful disaster management (a process that includes prevention, mitigation, preparedness, response, and recovery). Airborne data acquisition using helicopter and unmanned aerial vehicle (UAV) helps obtain a bird’s-eye view of disaster-affected areas. However, a major challenge to this approach is robustly processing a large amount of data to identify and map objects of interest on the ground in real-time. The current process is resource-intensive (must be carried out manually) and requires offline computing (through post-processing of aerial videos). This research introduces and evaluates a series of convolutional neural network (CNN) models for ground object detection from aerial views of disaster’s aftermath. These models are capable of recognizing critical ground assets including building roofs (both damaged and undamaged), vehicles, vegetation, debris, and flooded areas. The CNN models are trained on an in-house aerial video dataset (named Volan2018) that is created using web mining techniques. Volan2018 contains eight annotated aerial videos (65,580 frames) collected by drone or helicopter from eight different locations in various hurricanes that struck the United States in 2017–2018. Eight CNN models based on You-Only-Look-Once (YOLO) algorithm are trained by transfer learning, i.e., pre-trained on the COCO/VOC dataset and re-trained on Volan2018 dataset, and achieve 80.69% mAP for high altitude (helicopter footage) and 74.48% for low altitude (drone footage), respectively. This paper also presents a thorough investigation of the effect of camera altitude, data balance, and pre-trained weights on model performance, and finds that models trained and tested on videos taken from similar altitude outperform those trained and tested on videos taken from different altitudes. Moreover, the CNN model pre-trained on the VOC dataset and re-trained on balanced drone video yields the best result in significantly shorter training time.}
}
@article{ALBAHRI2024109409,
title = {A systematic review of trustworthy artificial intelligence applications in natural disasters},
journal = {Computers and Electrical Engineering},
volume = {118},
pages = {109409},
year = {2024},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2024.109409},
url = {https://www.sciencedirect.com/science/article/pii/S0045790624003379},
author = {A.S. Albahri and Yahya Layth Khaleel and Mustafa Abdulfattah Habeeb and Reem D. Ismael and Qabas A. Hameed and Muhammet Deveci and Raad Z. Homod and O.S. Albahri and A.H. Alamoodi and Laith Alzubaidi},
keywords = {Artificial intelligence, Natural disasters, Explainability, Data fusion, Taxonomy, Trustworthy},
abstract = {Artificial intelligence (AI) holds significant promise for advancing natural disaster management through the use of predictive models that analyze extensive datasets, identify patterns, and forecast potential disasters. These models facilitate proactive measures such as early warning systems (EWSs), evacuation planning, and resource allocation, addressing the substantial challenges associated with natural disasters. This study offers a comprehensive exploration of trustworthy AI applications in natural disasters, encompassing disaster management, risk assessment, and disaster prediction. This research is underpinned by an extensive review of reputable sources, including Science Direct (SD), Scopus, IEEE Xplore (IEEE), and Web of Science (WoS). Three queries were formulated to retrieve 981 papers from the earliest documented scientific production until February 2024. After meticulous screening, deduplication, and application of the inclusion and exclusion criteria, 108 studies were included in the quantitative synthesis. This study provides a specific taxonomy of AI applications in natural disasters and explores the motivations, challenges, recommendations, and limitations of recent advancements. It also offers an overview of recent techniques and developments in disaster management using explainable artificial intelligence (XAI), data fusion, data mining, machine learning (ML), deep learning (DL), fuzzy logic, and multicriteria decision-making (MCDM). This systematic contribution addresses seven open issues and provides critical solutions through essential insights, laying the groundwork for various future works in trustworthiness AI-based natural disaster management. Despite the potential benefits, challenges persist in the application of AI to natural disaster management. In these contexts, this study identifies several unused and used areas in natural disaster-based AI theory, collects the disaster datasets, ML, and DL techniques, and offers a valuable XAI approach to unravel the complex relationships and dynamics involved and the utilization of data fusion techniques in decision-making processes related to natural disasters. Finally, the study extensively analyzed ethical considerations, bias, and consequences in natural disaster-based AI.}
}
@article{KALAMKAR2023100327,
title = {Multimodal image fusion: A systematic review},
journal = {Decision Analytics Journal},
volume = {9},
pages = {100327},
year = {2023},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2023.100327},
url = {https://www.sciencedirect.com/science/article/pii/S2772662223001674},
author = {Shrida Kalamkar and Geetha Mary A.},
keywords = {Multimodal data, Image fusion, Deep learning, Transformers, Wavelet transforms},
abstract = {Multimodal image fusion combines information from multiple modalities to generate a composite image containing complementary information. Multimodal image fusion is challenging due to the heterogeneous nature of data, misalignment and nonlinear relationships between input data, or incomplete data during the fusion process. In recent years, several attention mechanisms have been introduced to enhance the performance of deep learning models. However, little literature is available on multimodal image fusion using attention mechanisms. This paper aims to study and analyze the latest deep-learning approaches, including attention mechanisms for multimodal image fusion. As a result of this study, the graphical taxonomy based on the different image modalities, various fusion strategies, fusion levels, and metrics for fusion tasks has been put forth. The focus has been on various Multimodal image fusion frameworks based on deep-learning techniques as their core methodology. This paper also sheds light on the challenges and future research directions in this field, application domains, and benchmark datasets used for multimodal fusion tasks. This paper contributes to the research on Multimodal image fusion and can help researchers select a suitable methodology for their applications.}
}
@article{BOUGUETTAYA2022108309,
title = {A review on early wildfire detection from unmanned aerial vehicles using deep learning-based computer vision algorithms},
journal = {Signal Processing},
volume = {190},
pages = {108309},
year = {2022},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2021.108309},
url = {https://www.sciencedirect.com/science/article/pii/S0165168421003467},
author = {Abdelmalek Bouguettaya and Hafed Zarzour and Amine Mohammed Taberkit and Ahmed Kechida},
keywords = {Computer vision, Deep learning, Aerial images processing, Wildfire detection system, Smoke detection system, Unmanned aerial vehicle},
abstract = {Wildfire is one of the most critical natural disasters that threaten wildlands and forest resources. Traditional firefighting systems, which are based on ground crew inspection, have several limits and can expose firefighters’ lives to danger. Thus, remote sensing technologies have become one of the most demanded strategies to fight against wildfires, especially UAV-based remote sensing technologies. They have been adopted to detect forest fires at their early stages, before becoming uncontrollable. Autonomous wildfire early detection from UAV-based visual data using different deep learning algorithms has attracted significant interest in the last few years. To this end, in this paper, we focused on wildfires detection at their early stages in forest and wildland areas, using deep learning-based computer vision algorithms to prevent and then reduce disastrous losses in terms of human lives and forest resources.}
}
@article{SCHIEFER2023100034,
title = {UAV-based reference data for the prediction of fractional cover of standing deadwood from Sentinel time series},
journal = {ISPRS Open Journal of Photogrammetry and Remote Sensing},
volume = {8},
pages = {100034},
year = {2023},
issn = {2667-3932},
doi = {https://doi.org/10.1016/j.ophoto.2023.100034},
url = {https://www.sciencedirect.com/science/article/pii/S2667393223000054},
author = {Felix Schiefer and Sebastian Schmidtlein and Annett Frick and Julian Frey and Randolf Klinke and Katarzyna Zielewska-Büttner and Samuli Junttila and Andreas Uhl and Teja Kattenborn},
keywords = {Reference data, Standing deadwood, Deep learning, Tree mortality, Upscaling},
abstract = {Increasing tree mortality due to climate change has been observed globally. Remote sensing is a suitable means for detecting tree mortality and has been proven effective for the assessment of abrupt and large-scale stand-replacing disturbances, such as those caused by windthrow, clear-cut harvesting, or wildfire. Non-stand replacing tree mortality events (e.g., due to drought) are more difficult to detect with satellite data – especially across regions and forest types. A common limitation for this is the availability of spatially explicit reference data. To address this issue, we propose an automated generation of reference data using uncrewed aerial vehicles (UAV) and deep learning-based pattern recognition. In this study, we used convolutional neural networks (CNN) to semantically segment crowns of standing dead trees from 176 UAV-based very high-resolution (<4 cm) RGB-orthomosaics that we acquired over six regions in Germany and Finland between 2017 and 2021. The local-level CNN-predictions were then extrapolated to landscape-level using Sentinel-1 (i.e., backscatter and interferometric coherence), Sentinel-2 time series, and long short term memory networks (LSTM) to predict the cover fraction of standing deadwood per Sentinel-pixel. The CNN-based segmentation of standing deadwood from UAV imagery was accurate (F1-score = 0.85) and consistent across the different study sites and years. Best results for the LSTM-based extrapolation of fractional cover of standing deadwood using Sentinel-1 and -2 time series were achieved using all available Sentinel-1 and --2 bands, kernel normalized difference vegetation index (kNDVI), and normalized difference water index (NDWI) (Pearson’s r = 0.66, total least squares regression slope = 1.58). The landscape-level predictions showed high spatial detail and were transferable across regions and years. Our results highlight the effectiveness of deep learning-based algorithms for an automated and rapid generation of reference data for large areas using UAV imagery. Potential for improving the presented upscaling approach was found particularly in ensuring the spatial and temporal consistency of the two data sources (e.g., co-registration of very high-resolution UAV data and medium resolution satellite data). The increasing availability of publicly available UAV imagery on sharing platforms combined with automated and transferable deep learning-based mapping algorithms will further increase the potential of such multi-scale approaches.}
}
@article{HOLZINGER202316,
title = {AI for life: Trends in artificial intelligence for biotechnology},
journal = {New Biotechnology},
volume = {74},
pages = {16-24},
year = {2023},
issn = {1871-6784},
doi = {https://doi.org/10.1016/j.nbt.2023.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1871678423000031},
author = {Andreas Holzinger and Katharina Keiblinger and Petr Holub and Kurt Zatloukal and Heimo Müller},
keywords = {Artificial Intelligence, Biotechnology, Deep Learning, Digital Transformation, Machine Learning},
abstract = {Due to popular successes (e.g., ChatGPT) Artificial Intelligence (AI) is on everyone's lips today. When advances in biotechnology are combined with advances in AI unprecedented new potential solutions become available. This can help with many global problems and contribute to important Sustainability Development Goals. Current examples include Food Security, Health and Well-being, Clean Water, Clean Energy, Responsible Consumption and Production, Climate Action, Life below Water, or protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss. AI is ubiquitous in the life sciences today. Topics include a wide range from machine learning and Big Data analytics, knowledge discovery and data mining, biomedical ontologies, knowledge-based reasoning, natural language processing, decision support and reasoning under uncertainty, temporal and spatial representation and inference, and methodological aspects of explainable AI (XAI) with applications of biotechnology. In this pre-Editorial paper, we provide an overview of open research issues and challenges for each of the topics addressed in this special issue. Potential authors can directly use this as a guideline for developing their paper.}
}
@article{CHENG20241,
title = {Methods and datasets on semantic segmentation for Unmanned Aerial Vehicle remote sensing images: A review},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {211},
pages = {1-34},
year = {2024},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2024.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0924271624000844},
author = {Jian Cheng and Changjian Deng and Yanzhou Su and Zeyu An and Qi Wang},
keywords = {Semantic segmentation, Unmanned aerial vehicle, Remote sensing images, Deep learning},
abstract = {Unmanned Aerial Vehicle (UAV) has seen a dramatic rise in popularity for remote-sensing image acquisition and analysis in recent years. It has brought promising results in low-altitude monitoring tasks that require detailed visual inspections. Semantic segmentation is one of the hot topics in UAV remote sensing image analysis, as its capability to mine contextual semantic information from UAV images is crucial for achieving a fine-grained understanding of scenes. However, in the remote sensing field, recent reviews have not focused on combining “UAV remote sensing” and “semantic segmentation” to summarize the advanced works and future trends. In this study, we focus primarily on describing various recent semantic segmentation methods applied in UAV remote sensing images and summarizing their advantages and limitations. According to the distinction in modeling contextual semantic information, we have categorized and outlined the methods based on graph-based contextual models and deep-learning-based models. Publicly available UAV-based image datasets are also gathered to encourage systematic research on advanced semantic segmentation methods. We provide quantitative results of representative methods on two high-resolution UAV-based image datasets for fair comparisons and discussions in terms of semantic segmentation accuracy and model inference efficiency. Besides, this paper concludes some remaining challenges and future directions in semantic segmentation for UAV remote sensing images and points out that methods based on deep learning will become the future research trend.}
}
@article{NELSON2024100008,
title = {Trends and applications in wildfire burned area mapping: Remote sensing data, cloud geoprocessing platforms, and emerging algorithms},
journal = {Geomatica},
volume = {76},
number = {1},
pages = {100008},
year = {2024},
issn = {1195-1036},
doi = {https://doi.org/10.1016/j.geomat.2024.100008},
url = {https://www.sciencedirect.com/science/article/pii/S1195103624000089},
author = {Daniel Martin Nelson and Yuhong He and G.W.K. Moore},
keywords = {Wildfire, Fire mapping, Cloud geoprocessing, Google earth engine, Unmanned aerial vehicles, Satellite platforms, Deep learning, Machine learning},
abstract = {Wildfires pose an increasing risk to expanding urban population centers, and to critical habitats for plant and animal species. Improving current wildland management strategies are vital to mitigating loss of global biodiversity and preventing the displacement of urban residents. Accurate maps of areas burned by wildfires is a primary source of information required for developing wildland management strategies. Advancements in underlying technologies for mapping wildfires comes from three key areas: 1) remotely sensed data, 2) cloud geoprocessing platforms, and 3) emerging image processing algorithms. Trends across these three areas were explored in this review, in addition to an in-depth discussion and comparison of optimal usage scenarios. This review provides crucial insights for researchers and practitioners keen on exploring emerging methods that hold the potential to improve wildfire burned area mapping procedures.}
}
@article{ZHAO2024100233,
title = {A bibliometric analysis using machine learning to track paradigm shifts and analytical advances in forest ecology and forestry journal publications from 2010 to 2022},
journal = {Forest Ecosystems},
volume = {11},
pages = {100233},
year = {2024},
issn = {2197-5620},
doi = {https://doi.org/10.1016/j.fecs.2024.100233},
url = {https://www.sciencedirect.com/science/article/pii/S2197562024000691},
author = {Jin Zhao and Liyu Li and Jian Liu and Yimei Yan and Qian Wang and Chris Newman and Youbing Zhou},
keywords = {Forest ecology, Forestry, R software, Structural topic modelling, Machine learning, Publication},
abstract = {Forest habitats are critical for biodiversity, ecosystem services, human livelihoods, and well-being. Capacity to conduct theoretical and applied forest ecology research addressing direct (e.g., deforestation) and indirect (e.g., climate change) anthropogenic pressures has benefited considerably from new field- and statistical-techniques. We used machine learning and bibliometric structural topic modelling to identify 20 latent topics comprising four principal fields from a corpus of 16,952 forest ecology/forestry articles published in eight ecology and five forestry journals between 2010 and 2022. Articles published per year increased from 820 in 2010 to 2,354 in 2021, shifting toward more applied topics. Publications from China and some countries in North America and Europe dominated, with relatively fewer articles from some countries in West and Central Africa and West Asia, despite globally important forest resources. Most study sites were in some countries in North America, Central Asia, and South America, and Australia. Articles utilizing R statistical software predominated, increasing from 29.5% in 2010 to 71.4% in 2022. The most frequently used packages included lme4, vegan, nlme, MuMIn, ggplot2, car, MASS, mgcv, multcomp and raster. R was more often used in forest ecology than applied forestry articles. R software offers advantages in script and workflow-sharing compared to other statistical packages. Our findings demonstrate that the disciplines of forest ecology/forestry are expanding both in number and scope, aided by more sophisticated statistical tools, to tackle the challenges of redressing forest habitat loss and the socio-economic impacts of deforestation.}
}
@article{BUCHELT2024121530,
title = {Exploring artificial intelligence for applications of drones in forest ecology and management},
journal = {Forest Ecology and Management},
volume = {551},
pages = {121530},
year = {2024},
issn = {0378-1127},
doi = {https://doi.org/10.1016/j.foreco.2023.121530},
url = {https://www.sciencedirect.com/science/article/pii/S0378112723007648},
author = {Alexander Buchelt and Alexander Adrowitzer and Peter Kieseberg and Christoph Gollob and Arne Nothdurft and Sebastian Eresheim and Sebastian Tschiatschek and Karl Stampfer and Andreas Holzinger},
keywords = {Artificial intelligence, Machine learning, Drones, Unmanned aerial vehicles, Forestry, Forest ecology},
abstract = {This paper highlights the significance of Artificial Intelligence (AI) in the realm of drone applications in forestry. Drones have revolutionized various forest operations, and their role in mapping, monitoring, and inventory procedures is explored comprehensively. Leveraging advanced imaging technologies and data processing techniques, drones enable real-time tracking of changes in forested landscapes, facilitating effective monitoring of threats such as fire outbreaks and pest infestations. They expedite forest inventory by swiftly surveying large areas, providing precise data on tree species identification, size estimation, and health assessment, thus supporting informed decision-making and sustainable forest management practices. Moreover, drones contribute to tree planting, pruning, and harvesting, while monitoring reforestation efforts in real-time. Wildlife monitoring is also enhanced, aiding in the identification of conservation concerns and informing targeted conservation strategies. Drones offer a safer and more efficient alternative in search and rescue operations within dense forests, reducing response time and improving outcomes. Additionally, drones equipped with thermal cameras enable early detection of wildfires, enabling timely response, mitigation, and preservation efforts. The integration of AI and drones holds immense potential for enhancing forestry practices and contributing to sustainable land management. In the future explainable AI (XAI) improves trust and safety by providing transparency in decision-making, aiding in liability issues, and enabling precise operations. XAI facilitates better environmental monitoring and impact analysis, contributing to efficient forest management and preservation efforts. If a drone's AI can explain its actions, it will be easier to understand why it chose a particular path or action, which could inform safety procedures and improvements.}
}
@article{CHENG2022111302,
title = {Data-driven surrogate model with latent data assimilation: Application to wildfire forecasting},
journal = {Journal of Computational Physics},
volume = {464},
pages = {111302},
year = {2022},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2022.111302},
url = {https://www.sciencedirect.com/science/article/pii/S0021999122003643},
author = {Sibo Cheng and I. Colin Prentice and Yuhan Huang and Yufang Jin and Yi-Ke Guo and Rossella Arcucci},
keywords = {Deep learning, Reduced-order modelling, Data assimilation, Wildfire forecasting, LSTM, Fire spread},
abstract = {The large and catastrophic wildfires have been increasing across the globe in the recent decade, highlighting the importance of simulating and forecasting fire dynamics in near real-time. This is extremely challenging due to the complexities of physical models and geographical features. Running physics-based simulations for large wildfire events in near real-time are computationally expensive, if not infeasible. In this work, we develop and test a novel data-model integration scheme for fire progression forecasting, that combines Reduced-order modelling, recurrent neural networks (Long-Short-Term Memory), data assimilation, and error covariance tuning. The Reduced-order modelling and the machine learning surrogate model ensure the efficiency of the proposed approach while the data assimilation enables the system to adjust the simulation with observations. We applied this algorithm to simulate and forecast three recent large wildfire events in California from 2017 to 2020. The deep-learning-based surrogate model runs around 1000 times faster than the Cellular Automata simulation which is used to generate training data-sets. The daily fire perimeters derived from satellite observation are used as observation data in Latent Assimilation to adjust the fire forecasting in near real-time. An error covariance tuning algorithm is also performed in the reduced space to estimate prior simulation and observation errors. The evolution of the averaged relative root mean square error (R-RMSE) shows that data assimilation and covariance tuning reduce the RMSE by about 50% and considerably improves the forecasting accuracy. As a first attempt at a reduced order wildfire spread forecasting, our exploratory work showed the potential of data-driven machine learning models to speed up fire forecasting for various applications.}
}
@article{ISTIAK2023102305,
title = {Adoption of Unmanned Aerial Vehicle (UAV) imagery in agricultural management: A systematic literature review},
journal = {Ecological Informatics},
volume = {78},
pages = {102305},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102305},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123003345},
author = {Md. Abrar Istiak and M.M. Mahbubul Syeed and Md Shakhawat Hossain and Mohammad Faisal Uddin and Mahady Hasan and Razib Hayat Khan and Nafis Saami Azad},
keywords = {Remote sensing, UAV, Unmanned aerial vehicle, Precision agriculture, Smart farming, Visual imagery, Deep learning, Systematic literature review},
abstract = {Precision agriculture and Smart farming have become the essential backbone for sustainable agricultural production by leveraging cutting edge remote sensing and communication technologies, meshed with AI driven data processing and decision making approaches. Agricultural segments, such as crop and livestock monitoring, crop/plant classification, yield prediction, weed detection, automatic harvesting, early detection, and prevention of diseases are being served for efficient, cost-effective process monitoring with increased profitability. With the remarkable development in recent decades, Unmanned Aerial Vehicles (UAV) based remote sensing technologies have gained rapid proliferation and exploitation in precision agriculture. Consequently, over the past decades, researchers have explored the capabilities of UAVs for real-time imagery data acquisition and processing through powerful Deep Learning (DL) algorithms to optimize agricultural process management. Being a prevalent research domain of high-tech field with constant advancement, there is a need for systematic review to recapitulate the contemporary literature and reveal the domain’s intellectual structure. This systematic literature review (SLR) research has methodically scrutinized 214 peer reviewed articles on the concerned domain that are published in ranked journals and conferences over the past 14 years. Several pressing dimensions are investigated, including, the feasibility assessment of the UAVs in precision agriculture, determine the impact of imaging modalities and imagery datasets in relation to agricultural applications, categorically evaluate the UAV configuration and offer detailed scrutiny of AI methods in relation to real-time control, decision making and action performance in agricultural applications. Alongside, the taxonomy of crops across the world is documented for which UAV is utilized. Finally, the main challenges and directions of future research along the track is presented.}
}
@article{KHAN2021115125,
title = {DeepSmoke: Deep learning model for smoke detection and segmentation in outdoor environments},
journal = {Expert Systems with Applications},
volume = {182},
pages = {115125},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115125},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421005662},
author = {Salman Khan and Khan Muhammad and Tanveer Hussain and Javier Del Ser and Fabio Cuzzolin and Siddhartha Bhattacharyya and Zahid Akhtar and Victor Hugo C. {de Albuquerque}},
keywords = {Smoke detection and segmentation, Semantic segmentation, Foggy surveillance environment, Wildfires, Disaster management},
abstract = {Fire disaster throughout the globe causes social, environmental, and economical damage, making its early detection and instant reporting essential for saving human lives and properties. Smoke detection plays a key role in early fire detection but majority of the existing methods are limited to either indoor or outdoor surveillance environments, with poor performance for hazy scenarios. In this paper, we present a Convolutional Neural Network (CNN)-based smoke detection and segmentation framework for both clear and hazy environments. Unlike existing methods, we employ an efficient CNN architecture, termed EfficientNet, for smoke detection with better accuracy. We also segment the smoke regions using DeepLabv3+, which is supported by effective encoders and decoders along with a pixel-wise classifier for optimum localization. Our smoke detection results evince a noticeable gain up to 3% in accuracy and a decrease of 0.46% in False Alarm Rate (FAR), while segmentation reports a significant increase of 2% and 1% in global accuracy and mean Intersection over Union (IoU) scores, respectively. This makes our method a best fit for smoke detection and segmentation in real-world surveillance settings.}
}
@incollection{TERMANINI2023177,
title = {Chapter 9 - The drone technology, transport strategy, and risk evaluation, and biomedical revolution},
editor = {Rocky Termanini},
booktitle = {Biomedical Defense Principles to Counter DNA Deep Hacking},
publisher = {Academic Press},
pages = {177-203},
year = {2023},
isbn = {978-0-323-99914-4},
doi = {https://doi.org/10.1016/B978-0-323-99914-4.00018-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323999144000187},
author = {Rocky Termanini},
keywords = {Autonomic payload, Bioinformatics, Cognitive performative, DNAxnet, DSD Unix, Internet of Nano Things (IoNT), Nano chip},
abstract = {Drones (Unmanned Aerial Vehicles) are becoming a highly strategic technology in modern military weaponry. Drones today are characterized by outstanding autonomic maneuverability (self-controlled) with built-in safeguards; Drones, as flying robots, are artificially intelligent and highly educated. Drones are computerized flying “slaves” that can achieve next-to-impossible services beyond human capability and skills. New generations of drones are actually hyper-drones that can outperform any system. Biohackers have adopted drone technology and have developed sophisticated smart systems to invade DNA integrity and Genome confidentiality. Aside from Laundry delivery, Domino Pizza's unmanned aerial modernization, self-window washing, and biohackers have introduced a new dimension to DNA invasion. An incredible missile called DNAxnet, which is the latest marvel of nano-engineering and way ahead of its time will be deployed in their most audacious dare-devil attacks. The chapter explains in detail the components and engineering design. Again, the chapter covers a review of the FBI's CODIS system and its remarkable intelligence in matching the most complex crimes to the slickest techno-criminals. The chapter presents an extensive strategy of bio-attack deployed in three different scenarios.}
}
@article{LIU2024100449,
title = {Remote sensing-enhanced transfer learning approach for agricultural damage and change detection: A deep learning perspective},
journal = {Big Data Research},
volume = {36},
pages = {100449},
year = {2024},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2024.100449},
url = {https://www.sciencedirect.com/science/article/pii/S221457962400025X},
author = {Zehua Liu and Jiuhao Li and Mahmood Ashraf and M.S. Syam and Muhammad Asif and Emad Mahrous Awwad and Muna Al-Razgan and Uzair Aslam Bhatti},
keywords = {Transfer learning, Damage detection, Remote sensing image},
abstract = {With the continuous advancement of science and technology, there has been a growing awareness of safety among people worldwide. Natural disasters such as wildfires, earthquakes, and floods pose persistent threats to both lives and property on our planet, which serves as our fundamental habitat. While it is impossible to prevent or entirely avert these calamities, rapid identification of affected areas and prompt damage assessment post-disaster can significantly aid in the formulation of effective rescue strategies, ultimately saving more lives. This article delves into the application of transfer learning in satellite image damage assessment—a methodology that involves transferring previously acquired knowledge to enhance a model's adaptability to new tasks. Given the limited availability of datasets for satellite image analysis, transfer learning proves to be an effective approach. Specifically, the study proposes a transfer learning method based on YOLOv5 for satellite image damage assessment. Initially, a general convolutional neural network model is trained using a substantial dataset of natural images. Subsequently, the early layers of this model are frozen, while the later layers undergo training to adapt to satellite image data. Fine-tuning is then employed to further enhance the overall model performance. The results demonstrate that this approach yields a high accuracy rate in satellite image damage assessment. Moreover, compared to conventional deep learning methods, the proposed method effectively leverages pre-trained models' knowledge, thereby reducing data dependency. Additionally, it displays robust generalization capabilities across diverse tasks and datasets, underscoring its potential for facilitating transfer learning across various domains.}
}
@article{NATH2022101450,
title = {Drone mapping of damage information in GPS-Denied disaster sites},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101450},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101450},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002020},
author = {Nipun D. Nath and Chih-Shen Cheng and Amir H. Behzadan},
keywords = {Disaster management, Unmanned aerial vehicle (UAV), Damage assessment, Homography, Photogrammetry, Spatial mapping},
abstract = {The increasing number and severity of natural hazard events in recent years, and their devastating impact on human life, local economies, and the built environment has called governments around the world into action and created a new mandate for a paradigm shift in disaster management and mitigation policies. To this end, new affordable technologies with mobile connectivity (e.g., smartphones, unmanned systems, reality capture devices) have scaled up tasks such as data collection and curation, leading to a significant increase in the volume of data gathered and shared in the aftermath of disasters. In the meantime, advancements in high-power and distributed computing have created new opportunities in fast and reliable data analytics. In particular to the application of drones in disaster response, past research has primarily focused on aerial data collection and more recently, ground object detection. Geolocalization of drone data (i.e., the process of determining the geographical position of objects in drone’s field of view), however, is a complex task that relies on prior knowledge of the drone’s geolocation (e.g., flight path coordinates, inertial sensors, camera gaze). Such metadata may not be always available or shared across platforms especially with the increased use of crowdsourcing in disaster response, damage assessment, and recovery. This paper presents a methodology for spatial mapping of disaster impact information in drone videos without reliance on GPS data of the aerial camera. We perform progressive mapping using scale-invariant visual features in red–greenblue (RGB) videos of disaster-affected sites in two major hurricanes in North America, namely Harvey (2017) and Dorian (2019). Results indicate that the proposed methodology can project objects from the perspective view of a drone camera onto an orthogonal map with 32.7–36.9 ft of average root mean square (RMS) error in a land area of 18–45 acres.}
}
@article{DEZARZA2023553,
title = {Area Estimation of Forest Fires using TabNet with Transformers},
journal = {Procedia Computer Science},
volume = {225},
pages = {553-563},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.040},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923011985},
author = {I. {de Zarzà} and J. {de Curtò} and Carlos T. Calafate},
keywords = {forest fires, area estimation, TabNet, Transformers, feature engineering, tabular data},
abstract = {In this paper, we propose a novel approach for estimating the burned area of forest fires using the TabNet transformer-based architecture. Forest fires pose a significant threat to ecosystems, and accurate estimation of the affected area is essential for effective disaster management and resource allocation. We conducted a comprehensive analysis of various Machine Learning (ML) and Deep Learning (DL) methods, including Random Forest, Neural Networks, Neural Architecture Search (NAS), TabNet with Transformers, and Self-Supervised Learning with Autoencoders, to identify the most accurate and efficient model for area estimation. Our experiments employed a publicly available dataset, UCI Forest Fires, containing a combination of meteorological, geospatial, and categorical data. We implemented a thorough preprocessing pipeline that included handling categorical variables, standardization, and feature engineering. The results demonstrate that TabNet outperforms other methods, achieving state-of-the-art accuracy and generalization in predicting the target variable with a Mean Squared Error (MSE) of 2319 in training and 7781 in testing.}
}
@article{BHATTI2024100465,
title = {Utilizing convolutional neural networks (CNN) and U-Net architecture for precise crop and weed segmentation in agricultural imagery: A deep learning approach},
journal = {Big Data Research},
volume = {36},
pages = {100465},
year = {2024},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2024.100465},
url = {https://www.sciencedirect.com/science/article/pii/S2214579624000418},
author = {Mughair Aslam Bhatti and M.S. Syam and Huafeng Chen and Yurong Hu and Li Wai Keung and Zeeshan Zeeshan and Yasser A. Ali and Nadia Sarhan},
keywords = {Crop weed, Image segmentation, U-NET deep learning},
abstract = {This study presents the implementation and evaluation of a convolutional neural network (CNN) based image segmentation model using the U-Net architecture for forest image segmentation. The proposed algorithm starts by preprocessing the datasets of satellite images and corresponding masks from a repository source. Data preprocessing involves resizing, normalizing, and splitting the images and masks into training and testing datasets. The U-Net model architecture, comprising encoder and decoder parts with skip connections, is defined and compiled with binary cross-entropy loss and Adam optimizer. Training includes early stopping and checkpoint saving mechanisms to prevent overfitting and retain the best model weights. Evaluation metrics such as Intersection over Union (IoU), Dice coefficient, pixel accuracy, precision, recall, specificity, and F1-score are computed to assess the model's performance. Visualization of results includes comparing predicted segmentation masks with ground truth masks for qualitative analysis. The study emphasizes the importance of training data size in achieving accurate segmentation models and highlights the potential of U-Net architecture for forest image segmentation tasks.}
}
@article{AMIRI2024132827,
title = {Comprehensive survey of artificial intelligence techniques and strategies for climate change mitigation},
journal = {Energy},
volume = {308},
pages = {132827},
year = {2024},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2024.132827},
url = {https://www.sciencedirect.com/science/article/pii/S036054422402601X},
author = {Zahra Amiri and Arash Heidari and Nima Jafari Navimipour},
keywords = {Climate change, Artificial intelligence, Systematic literature review, Machine learning, Deep learning},
abstract = {With the galloping progress of the changing climates all around the world, Machine Learning (ML) approaches have been prevalently studied in many types of research in this area. ML is a robust tool for acquiring perspectives from data. In this paper, we elaborate on climate change mitigation issues and ML approaches leveraged to solve these issues and aid in the improvement and function of sustainable energy systems. ML has been employed in multiple applications and many scopes of climate subjects such as ecosystems, agriculture, buildings and cities, industry, and transportation. So, a Systematic Literature Review (SLR) is applied to explore and evaluate findings from related research. In this paper, we propose a novel taxonomy of Deep Learning (DL) method applications for climate change mitigation, a comprehensive analysis that has not been conducted before. We evaluated these methods based on critical parameters such as accuracy, scalability, and interpretability and quantitatively compared their results. This analysis provides new insights into the effectiveness and reliability of DL methods in addressing climate change challenges. We classified climate change ML methods into six key customizable groups: ecosystems, industry, buildings and cities, transportation, agriculture, and hybrid applications. Afterward, state-of-the-art research on ML mechanisms and applications for climate change mitigation issues has been highlighted. In addition, many problems and issues related to ML implementation for climate change have been mapped, which are predicted to stimulate more researchers to manage the future disastrous effects of climate change. Based on the findings, most of the papers utilized Python as the most common simulation environment 38.5 % of the time. In addition, most of the methods were analyzed and evaluated in terms of some parameters, namely accuracy, latency, adaptability, and scalability, respectively. Lastly, classification is the most frequent ML task within climate change mitigation, accounting for 40 % of the total. Furthermore, Convolutional Neural Networks (CNNs) are the most widely utilized approach for a variety of applications.}
}
@article{GRAGNANIELLO2024124783,
title = {Fire and smoke detection from videos: A literature review under a novel taxonomy},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124783},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124783},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424016506},
author = {Diego Gragnaniello and Antonio Greco and Carlo Sansone and Bruno Vento},
keywords = {Fire detection, Smoke, Flame, Image, Video, Camera, Survey, Review},
abstract = {The recent development of deep learning based fire detection techniques and the availability of smart cameras able to execute these algorithms on the edge paved the way for sophisticated and efficient video-based firefighting systems. However, the limited available data to train these algorithms cast shadows on their robustness and generalization capability. In this survey, we review 153 papers published in the literature and 17 publicly available fire detection datasets with the aim of identifying application scenarios that better describe real-world fire detection challenges. In the proposed taxonomy, these are characterized by two features: i) the fire size in the framed scene that depends on several parameters, foremost the distance from the fire but also the camera optic; ii) the background activity, due to the presence of moving objects that may mislead the detector. On this basis, we analyzed the existing methods under a common scheme according to this new taxonomy and matched the solutions with the needs of specific application scenarios. Similarly, for 9 interesting video datasets acquired from cameras, we labeled 536 videos according to the proposed taxonomy and shared these annotations with the community. The aim of this fire detection review is two-fold: on one hand, we classify the existing scientific works according to the real application scenarios, determining the features that are promising in specific operative conditions; on the other hand, we provide a detailed analysis and annotation of available datasets to promote the development of more reliable validation protocols and the collection of data from missing scenarios.}
}
@article{GHARRAD2021108282,
title = {A five-step drone collaborative planning approach for the management of distributed spatial events and vehicle notification using multi-agent systems and firefly algorithms},
journal = {Computer Networks},
volume = {198},
pages = {108282},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108282},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621003029},
author = {Hana Gharrad and Nafaa Jabeur and Ansar Ul-Haque Yasar and Stephane Galland and Mohammed Mbarki},
keywords = {Drone collaboration, Collaborative planning, Multi-agent systems, Firefly algorithm, Intelligent transportation system},
abstract = {In spite of the performance that existing approaches for drone collaborative planning have demonstrated, there is still a need for new solutions which are capable of effectively identifying the right tasks for the right drones at the right times while maximizing the total benefits obtained from the drones’ actions. These new solutions should be particularly tested within the context of intelligent transportation systems to assess their impact on mobility and traffic flow. In order to address these issues, we present in this paper a new a five-step solution for drone collaborative planning. Our solution uses a Multi-Agent System as well as a Firefly Algorithm solution to enable drones jointly neutralize ongoing events by considering trust factors and cost/benefit analysis. The solution, which is also capable of issuing appropriate warnings to vehicles to prevent them from incurring any undesirable/dangerous impact due to ongoing events, is using a reward-driven competition to encourage drones to join collaborating teams. Our simulations are showing promising results in terms of processing time, energy consumption, and total reward obtained compared to two other planning approaches relaying on random and priority-based selection of the next locations that drones will visit respectively.}
}
@article{LI2023104653,
title = {Machine learning and remote sensing integration for leveraging urban sustainability: A review and framework},
journal = {Sustainable Cities and Society},
volume = {96},
pages = {104653},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2023.104653},
url = {https://www.sciencedirect.com/science/article/pii/S2210670723002640},
author = {Fei Li and Tan Yigitcanlar and Madhav Nepal and Kien Nguyen and Fatih Dur},
keywords = {Machine learning, Deep learning, Remote sensing, Sustainable urban development, Urban studies, Urban sustainability, Urban analytics},
abstract = {Climate change and rapid urbanisation exacerbated multiple urban issues threatening urban sustainability. Numerous studies integrated machine learning and remote sensing to monitor urban issues and develop mitigation strategies for sustainability. However, few studies comparatively analysed joint applications of machine learning and remote sensing for urban issues and sustainability. This paper presents a systematic review and formulates a framework integrating machine learning and remote sensing in urban studies. The literature analysis reveals: Most studies occurred in Asia, Europe, and North America, driven by technical and ethical factors, highlighting responsible approaches for data-scarce regions; Reviewed studies prioritised physical spatial aspects over socioeconomic factors, requiring multi-source data for comprehensive analysis; Conventional satellite, aerial images, and Lidar data are prevalent due to affordability, quality, and accessibility; Although supervised machine learning dominates, unsupervised methods and algorithm selection paradigms require exploration; Integration offers accurate results and thorough analysis in image processing and analytics, while image acquisition and decision-making necessitate human supervision. This paper provides a comprehensive review and an integrative framework for machine learning and remote sensing, enriching insights into their potential for urban studies and spatial analytics. The study informs urban planning and policymaking by promoting efficient management via enhanced machine learning and remote sensing integration, and bolstering data-driven decision-making.}
}
@article{WANG2023103891,
title = {Automatic real-time fire distance, size and power measurement driven by stereo camera and deep learning},
journal = {Fire Safety Journal},
volume = {140},
pages = {103891},
year = {2023},
issn = {0379-7112},
doi = {https://doi.org/10.1016/j.firesaf.2023.103891},
url = {https://www.sciencedirect.com/science/article/pii/S0379711223001595},
author = {Zilong Wang and Yifei Ding and Tianhang Zhang and Xinyan Huang},
keywords = {Fire calorimetry, Object detection, Computer vision, Heat release rate, Smart firefighting},
abstract = {Automatic real-time fire characterization is a crucial requirement of future smart firefighting. This work proposes a novel computer vision method to automatically measure the fire heat release rate, even when the camera is moving in real-time. Firstly, a portable binocular stereo camera is used to capture the real-time fire video stream that is fed into a pre-trained computer-vision model frame-by-frame to detect the fire region. By identifying the fire location inside the image, the real-time distance between the camera and the fire source is determined. This fire distance helps re-scale the images to match the input scale of the AI-image Fire Calorimetry. Then, the deep learning model can automatically output the transient fire power in real time. Results show that the stereo vision system is capable of accurately measuring the distance between the camera and the fire source, flame height, and power, with a relative error of less than 20%. This work provides an automatic and flexible way to measure the distance, power and hazard of fire in real-time, and such a method has broad applications in firefighting operations and decision-making.}
}
@article{MISHRA2024171713,
title = {Spatial analysis and machine learning prediction of forest fire susceptibility: a comprehensive approach for effective management and mitigation},
journal = {Science of The Total Environment},
volume = {926},
pages = {171713},
year = {2024},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2024.171713},
url = {https://www.sciencedirect.com/science/article/pii/S0048969724018552},
author = {Manoranjan Mishra and Rajkumar Guria and Biswaranjan Baraj and Ambika Prasad Nanda and Celso Augusto Guimarães Santos and Richarde Marques da Silva and FX Anjar Tri Laksono},
keywords = {Susceptibility map, Climate and anthropogenic factors, Spatiotemporal trends, Machine learning, Wildfire management},
abstract = {Forest fires (FF) in tropical seasonal forests impact ecosystem. Addressing FF in tropical ecosystems has become a priority to mitigate impacts on biodiversity loss and climate change. The escalating frequency and intensity of FF globally have become a mounting concern. Understanding their tendencies, patterns, and vulnerabilities is imperative for conserving ecosystems and facilitating the development of effective prevention and management strategies. This study investigates the trends, patterns, and spatiotemporal distribution of FF for the period of 2001–2022, and delineates the forest fire susceptibility zones in Odisha State, India. The study utilized: (a) MODIS imagery to examine active fire point data; (b) Kernel density tools; (c) FF risk prediction using two machine learning algorithms, namely Support Vector Machine (SVM) and Random Forest (RF); (d) Receiver Operating Characteristic and Area Under the Curve, along with various evaluation metrics; and (e) a total of 19 factors, including three topographical, seven climatic, four biophysical, and five anthropogenic, to create a map indicating areas vulnerable to FF. The validation results revealed that the RF model achieved a precision exceeding 94 % on the validation datasets, while the SVM model reached 89 %. The estimated forest fire susceptibility zones using RF and SVM techniques indicated that 20.14 % and 16.72 % of the area, respectively, fall under the “Very High Forest Fire” susceptibility class. Trend analysis reveals a general upward trend in forest fire occurrences (R2 = 0.59), with a notable increase after 2015, peaking in 2021. Notably, Angul district was identified as the most affected area, documenting the highest number of forest fire incidents over the past 22 years. Additionally, forest fire mitigation plans have been developed by drawing insights from forest fire management strategies implemented in various countries worldwide. Overall, this analysis provides valuable insights for policymakers and forest management authorities to develop effective strategies for forest fire prevention and mitigation.}
}
@article{HARKAT2023118594,
title = {Fire images classification based on a handcraft approach},
journal = {Expert Systems with Applications},
volume = {212},
pages = {118594},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118594},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422016499},
author = {Houda Harkat and José M.P. Nascimento and Alexandre Bernardino and Hasmath Farhana Thariq Ahmed},
keywords = {Wildfire, Higher-order features, Feature selection, Support Vector Machine (SVM), Radial Basis Function (RBF)},
abstract = {In recent years, wildfires and forest fires have ravaged millions of hectares of the forest all over the world. Recent technological breakthroughs have increased interest in computer vision-based fire classification that classifies fire and non-fire pixels from image or video datasets. Fire pixels from an image or video can be classified using either a traditional machine learning approach or a deep learning approach. Presently, the deep learning approach is the mainstream in forest fire detection studies. Although deep learning algorithms can handle vast amounts of data, they ignore the variation in complexity among training samples and as a result, their training model performance is limited. Furthermore, deep learning approaches with little data and features perform poorly in real-world challenging fire scenarios. As a result, the current study adopts a machine learning technique to extract higher-order features from the processed images from the publicly available datasets: Corsican dataset and FLAME, and a private dataset: Firefront_Gestosa, for classifying fire and non-fire pixels. It should be emphasized that in machine learning applications, handling multidimensional data to train a model is challenging. Feature selection is used to overcome this problem by removing redundant or irrelevant data that has an impact on the model's performance. In this paper, information-theoretic feature selection approaches are used to choose the most important features for classification while minimizing the computational cost. The traditional machine classifier, Support Vector Machine (SVM) is adopted in the present work, that works on the discriminative features input selected from the feature selection technique. The SVM performs the classification of fire and non-fire pixels with a Radial Basis Function (RBF) kernel, and the model's performance is measured using assessment measures such as overall accuracy, sensitivity, specificity, precision, recall, F-measure, and G-mean. The model draws an overall accuracy of 96,21%, a sensitivity of 94,42%, a specificity of 97,99%, a precision of 97,91%, a recall of 94,42%, an f-measure and g-mean values of 96,13% and 96,19% respectively.}
}